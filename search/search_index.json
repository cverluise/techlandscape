{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"techLandscape \u00b6 Welcome to the techLandscape documentation website. techLandscape is the data backbone of Verluise and Bergeaud (2021). BibTeX @ te chrepor t { verluiseBergeaud 2021 , t i tle = {}, au t hor= { Verluise , Cyril a n d Bergeaud , A nt o n i n }, year= { 2021 } } Chicago Verluise, Cyril and Antonin, Bergeaud . \"\". 2021 Here, you can find the project and API documentation. We open source the code to support future extensions and a collaborative way to create and continuously improve research databases. techLandscape is due to expand and improve continuously in the coming years. Make sure you receive updates, join our newsletter and star the GitHub repository!","title":"About"},{"location":"#techlandscape","text":"Welcome to the techLandscape documentation website. techLandscape is the data backbone of Verluise and Bergeaud (2021). BibTeX @ te chrepor t { verluiseBergeaud 2021 , t i tle = {}, au t hor= { Verluise , Cyril a n d Bergeaud , A nt o n i n }, year= { 2021 } } Chicago Verluise, Cyril and Antonin, Bergeaud . \"\". 2021 Here, you can find the project and API documentation. We open source the code to support future extensions and a collaborative way to create and continuously improve research databases. techLandscape is due to expand and improve continuously in the coming years. Make sure you receive updates, join our newsletter and star the GitHub repository!","title":"techLandscape"},{"location":"API_ANTISEED/","text":"get_aug_antiseed ( primary_key , tech_class , pc_list , size , table_ref , destination_table , credentials , write_disposition = 'WRITE_APPEND' , verbose = False ) \u00b6 Deprecated The augmented antiseed is not used any more. We have \"close\" negative examples from human annotations. Return the augmented anti-seed Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required pc_list str list of technological classes to draw from, comma-separated (e.g. A,B,C) required size int size of the antiseed required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path credentials file path required write_disposition str BQ write disposition 'WRITE_APPEND' verbose bool verbosity False Usage: techlandscape antiseed get-af-antiseed family_id cpc Y12,Y10 300 <expansion-table> <destination-table> credentials_bq.json Source code in techlandscape/antiseed.py @app . command ( deprecated = True ) def get_aug_antiseed ( primary_key : PrimaryKey , tech_class : TechClass , pc_list : str , size : int , table_ref : str , destination_table : str , credentials : Path , write_disposition : str = \"WRITE_APPEND\" , verbose : bool = False , ) -> None : \"\"\" !!! warning \"Deprecated\" The augmented antiseed is not used any more. We have \"close\" negative examples from human annotations. Return the augmented anti-seed Arguments: primary_key: table primary key tech_class: technological class considered pc_list: list of technological classes to draw from, comma-separated (e.g. A,B,C) size: size of the antiseed table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: credentials file path write_disposition: BQ write disposition verbose: verbosity **Usage:** ```shell techlandscape antiseed get-af-antiseed family_id cpc Y12,Y10 300 <expansion-table> <destination-table> credentials_bq.json ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) pc_list = pc_list . split ( \",\" ) pc_like_clause_ = get_pc_like_clause ( tech_class , pc_list , sub_group = True ) country_prefix = get_country_prefix ( primary_key ) query = f \"\"\" SELECT DISTINCT(r. { primary_key . value } ) AS { primary_key . value } , \"ANTISEED-AUG\" AS expansion_level FROM ` { project_id } .patents.publications` AS r, UNNEST( { tech_class . value } ) AS { tech_class . value } { country_prefix } LEFT OUTER JOIN { table_ref } AS tmp ON r. { primary_key . value } = tmp. { primary_key . value } WHERE { pc_like_clause_ } ORDER BY RAND() LIMIT { size } \"\"\" get_bq_job_done ( query , destination_table , credentials , write_disposition , verbose )","title":"antiseed"},{"location":"API_ANTISEED/#techlandscape.antiseed.get_aug_antiseed","text":"Deprecated The augmented antiseed is not used any more. We have \"close\" negative examples from human annotations. Return the augmented anti-seed Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required pc_list str list of technological classes to draw from, comma-separated (e.g. A,B,C) required size int size of the antiseed required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path credentials file path required write_disposition str BQ write disposition 'WRITE_APPEND' verbose bool verbosity False Usage: techlandscape antiseed get-af-antiseed family_id cpc Y12,Y10 300 <expansion-table> <destination-table> credentials_bq.json Source code in techlandscape/antiseed.py @app . command ( deprecated = True ) def get_aug_antiseed ( primary_key : PrimaryKey , tech_class : TechClass , pc_list : str , size : int , table_ref : str , destination_table : str , credentials : Path , write_disposition : str = \"WRITE_APPEND\" , verbose : bool = False , ) -> None : \"\"\" !!! warning \"Deprecated\" The augmented antiseed is not used any more. We have \"close\" negative examples from human annotations. Return the augmented anti-seed Arguments: primary_key: table primary key tech_class: technological class considered pc_list: list of technological classes to draw from, comma-separated (e.g. A,B,C) size: size of the antiseed table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: credentials file path write_disposition: BQ write disposition verbose: verbosity **Usage:** ```shell techlandscape antiseed get-af-antiseed family_id cpc Y12,Y10 300 <expansion-table> <destination-table> credentials_bq.json ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) pc_list = pc_list . split ( \",\" ) pc_like_clause_ = get_pc_like_clause ( tech_class , pc_list , sub_group = True ) country_prefix = get_country_prefix ( primary_key ) query = f \"\"\" SELECT DISTINCT(r. { primary_key . value } ) AS { primary_key . value } , \"ANTISEED-AUG\" AS expansion_level FROM ` { project_id } .patents.publications` AS r, UNNEST( { tech_class . value } ) AS { tech_class . value } { country_prefix } LEFT OUTER JOIN { table_ref } AS tmp ON r. { primary_key . value } = tmp. { primary_key . value } WHERE { pc_like_clause_ } ORDER BY RAND() LIMIT { size } \"\"\" get_bq_job_done ( query , destination_table , credentials , write_disposition , verbose )","title":"get_aug_antiseed()"},{"location":"API_ASSETS/","text":"get_publications_family ( destination_table , credentials , verbose = False ) \u00b6 Emulate patents-public-data.patents.publications table at the family_id level Parameters: Name Type Description Default destination_table str the BQ destination table ( project.dataset.table ) required credentials Path BQ credentials file path required verbose bool verbosity False Usage: techlandscape assets get-publications-family <your-table> <your-credentials> Note It takes up to 2 minutes to complete Source code in techlandscape/assets.py @app . command () @monitor def get_publications_family ( destination_table : str , credentials : Path , verbose : bool = False ): \"\"\"Emulate `patents-public-data.patents.publications` table at the *family_id* level Arguments: destination_table: the BQ destination table (`project.dataset.table`) credentials: BQ credentials file path verbose: verbosity **Usage:** ```shell techlandscape assets get-publications-family <your-table> <your-credentials> ``` !!! note It takes up to 2 minutes to complete \"\"\" query = f \"\"\" WITH fam AS ( SELECT DISTINCT family_id FROM `patents-public-data.patents.publications` ), crossover AS ( SELECT publication_number, family_id FROM `patents-public-data.patents.publications` ), pub AS ( SELECT family_id, MIN(publication_date) AS publication_date, SPLIT(STRING_AGG(DISTINCT(p.publication_number))) AS publication_number, SPLIT(STRING_AGG(DISTINCT(country_code))) AS country_code FROM `patents-public-data.patents.publications` AS p GROUP BY family_id ), tech_class AS ( SELECT family_id, [STRUCT(SPLIT(STRING_AGG(DISTINCT(cpc.code))) AS code)] AS cpc, [STRUCT(SPLIT(STRING_AGG(DISTINCT(ipc.code))) AS code)] AS ipc FROM `patents-public-data.patents.publications` AS p, UNNEST(cpc) AS cpc, UNNEST(ipc) AS ipc GROUP BY family_id ), cit AS ( SELECT p.family_id, [STRUCT(SPLIT(STRING_AGG(DISTINCT(crossover.family_id))) AS family_id)] AS citation FROM `patents-public-data.patents.publications` AS p, UNNEST(citation) AS citation LEFT JOIN crossover ON citation.publication_number = crossover.publication_number GROUP BY p.family_id), tmp_gpr AS ( SELECT family_id, SPLIT(STRING_AGG(DISTINCT(cited_by.publication_number))) AS cited_by_publication_number, CONCAT(ANY_VALUE(title), \" \\\\ n\", ANY_VALUE(abstract)) AS abstract #ANY_VALUE(abstract) AS abstract FROM `patents-public-data.google_patents_research.publications` AS p, UNNEST(cited_by) AS cited_by LEFT JOIN crossover ON p.publication_number = crossover.publication_number GROUP BY family_id), gpr AS ( SELECT tmp_gpr.family_id, ANY_VALUE(abstract) AS abstract, [STRUCT(SPLIT(STRING_AGG(DISTINCT(crossover.family_id))) AS family_id)] AS cited_by #SPLIT(STRING_AGG(DISTINCT(cited_by_publication_number))) AS publication_number)] AS cited_by FROM tmp_gpr, UNNEST(cited_by_publication_number) AS cited_by_publication_number LEFT JOIN crossover ON cited_by_publication_number = crossover.publication_number GROUP BY tmp_gpr.family_id) SELECT fam.family_id, pub.* EXCEPT(family_id), tech_class.* EXCEPT(family_id), cit.* EXCEPT(family_id), gpr.* EXCEPT(family_id) FROM fam LEFT JOIN pub ON fam.family_id = pub.family_id LEFT JOIN tech_class ON fam.family_id = tech_class.family_id LEFT JOIN cit ON fam.family_id = cit.family_id LEFT JOIN gpr ON fam.family_id = gpr.family_id \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"assets"},{"location":"API_ASSETS/#techlandscape.assets.get_publications_family","text":"Emulate patents-public-data.patents.publications table at the family_id level Parameters: Name Type Description Default destination_table str the BQ destination table ( project.dataset.table ) required credentials Path BQ credentials file path required verbose bool verbosity False Usage: techlandscape assets get-publications-family <your-table> <your-credentials> Note It takes up to 2 minutes to complete Source code in techlandscape/assets.py @app . command () @monitor def get_publications_family ( destination_table : str , credentials : Path , verbose : bool = False ): \"\"\"Emulate `patents-public-data.patents.publications` table at the *family_id* level Arguments: destination_table: the BQ destination table (`project.dataset.table`) credentials: BQ credentials file path verbose: verbosity **Usage:** ```shell techlandscape assets get-publications-family <your-table> <your-credentials> ``` !!! note It takes up to 2 minutes to complete \"\"\" query = f \"\"\" WITH fam AS ( SELECT DISTINCT family_id FROM `patents-public-data.patents.publications` ), crossover AS ( SELECT publication_number, family_id FROM `patents-public-data.patents.publications` ), pub AS ( SELECT family_id, MIN(publication_date) AS publication_date, SPLIT(STRING_AGG(DISTINCT(p.publication_number))) AS publication_number, SPLIT(STRING_AGG(DISTINCT(country_code))) AS country_code FROM `patents-public-data.patents.publications` AS p GROUP BY family_id ), tech_class AS ( SELECT family_id, [STRUCT(SPLIT(STRING_AGG(DISTINCT(cpc.code))) AS code)] AS cpc, [STRUCT(SPLIT(STRING_AGG(DISTINCT(ipc.code))) AS code)] AS ipc FROM `patents-public-data.patents.publications` AS p, UNNEST(cpc) AS cpc, UNNEST(ipc) AS ipc GROUP BY family_id ), cit AS ( SELECT p.family_id, [STRUCT(SPLIT(STRING_AGG(DISTINCT(crossover.family_id))) AS family_id)] AS citation FROM `patents-public-data.patents.publications` AS p, UNNEST(citation) AS citation LEFT JOIN crossover ON citation.publication_number = crossover.publication_number GROUP BY p.family_id), tmp_gpr AS ( SELECT family_id, SPLIT(STRING_AGG(DISTINCT(cited_by.publication_number))) AS cited_by_publication_number, CONCAT(ANY_VALUE(title), \" \\\\ n\", ANY_VALUE(abstract)) AS abstract #ANY_VALUE(abstract) AS abstract FROM `patents-public-data.google_patents_research.publications` AS p, UNNEST(cited_by) AS cited_by LEFT JOIN crossover ON p.publication_number = crossover.publication_number GROUP BY family_id), gpr AS ( SELECT tmp_gpr.family_id, ANY_VALUE(abstract) AS abstract, [STRUCT(SPLIT(STRING_AGG(DISTINCT(crossover.family_id))) AS family_id)] AS cited_by #SPLIT(STRING_AGG(DISTINCT(cited_by_publication_number))) AS publication_number)] AS cited_by FROM tmp_gpr, UNNEST(cited_by_publication_number) AS cited_by_publication_number LEFT JOIN crossover ON cited_by_publication_number = crossover.publication_number GROUP BY tmp_gpr.family_id) SELECT fam.family_id, pub.* EXCEPT(family_id), tech_class.* EXCEPT(family_id), cit.* EXCEPT(family_id), gpr.* EXCEPT(family_id) FROM fam LEFT JOIN pub ON fam.family_id = pub.family_id LEFT JOIN tech_class ON fam.family_id = tech_class.family_id LEFT JOIN cit ON fam.family_id = cit.family_id LEFT JOIN gpr ON fam.family_id = gpr.family_id \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"get_publications_family()"},{"location":"API_CANDIDATES/","text":"QueryCandidates \u00b6 Class get_query ( self ) \u00b6 Return candidates query (all) Source code in techlandscape/candidates.py def get_query ( self ) -> str : \"\"\"Return candidates query (all)\"\"\" query_patents = self . get_query_patents ( self . patents ) query_cpcs = self . get_query_cpcs ( self . cpcs ) query_keywords = self . get_query_keywords ( self . keywords ) query = f \"\"\" WITH tmp AS ( { \"UNION ALL\" . join ([ query_patents , query_cpcs , query_keywords ]) } ) SELECT p.family_id, STRING_AGG(tmp.publication_number) AS publication_number, CONCAT(ANY_VALUE(tmp.title), \" \\\\ n \\\\ n\", ANY_VALUE(tmp.abstract)) AS text, STRING_AGG(DISTINCT(tmp.match), \",\" order by tmp.match ASC ) AS match, ARRAY_LENGTH(SPLIT(STRING_AGG(tmp.match))) AS match_number, FROM tmp LEFT JOIN `patents-public-data.patents.publications` AS p ON tmp.publication_number=p.publication_number GROUP BY family_id \"\"\" return query get_query_cpcs ( self , cpcs ) \u00b6 Return candidates query based on cpcs Source code in techlandscape/candidates.py def get_query_cpcs ( self , cpcs : List [ str ]) -> str : \"\"\"Return candidates query based on cpcs\"\"\" cpcs = cpcs if cpcs else self . cpcs query = f \"\"\" SELECT publication_number, title, abstract, \"cpc\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr, UNNEST(cpc) as cpc WHERE { \" OR \" . join ( map ( lambda x : 'cpc.code LIKE \"' + x + '%\"' , cpcs )) } \"\"\" return query get_query_keywords ( self , keywords ) \u00b6 Return candidates query based on keywords Source code in techlandscape/candidates.py def get_query_keywords ( self , keywords : List [ str ]) -> str : \"\"\"Return candidates query based on keywords\"\"\" keywords = keywords if keywords else self . keywords query = f \"\"\" SELECT publication_number, title, abstract, \"keyword\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr WHERE { \" OR \" . join ( map ( lambda x : 'LOWER(gpr.abstract) LIKE \"%' + x + '%\"' , keywords )) } \"\"\" return query get_query_patents ( self , patents ) \u00b6 Return candidates query based on patent similarity Source code in techlandscape/candidates.py def get_query_patents ( self , patents : List [ str ]) -> str : \"\"\"Return candidates query based on patent similarity\"\"\" patents = patents if patents else self . patents query = f \"\"\" WITH SIMILAR AS ( SELECT similar.publication_number, \"patent\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr, UNNEST(similar) AS similar WHERE gpr.publication_number IN ( { \",\" . join ( map ( lambda x : '\"' + x + '\"' , patents )) } ) ) SELECT SIMILAR.publication_number, title, abstract, SIMILAR.match FROM `patents-public-data.google_patents_research.publications` AS gpr, SIMILAR WHERE SIMILAR.publication_number=gpr.publication_number AND SIMILAR.publication_number IS NOT NULL \"\"\" return query get_candidates ( config , destination_table , credentials , verbose = False ) \u00b6 Return seed candidates based on config . Candidate table is saved to destination_table Source code in techlandscape/candidates.py @app . command () def get_candidates ( config : Path , destination_table : str , credentials : Path , verbose : bool = False ): \"\"\" Return seed candidates based on `config`. Candidate table is saved to `destination_table` \"\"\" query = QueryCandidates ( config ) . get_query () get_bq_job_done ( query , destination_table , credentials , verbose = verbose ) get_candidates_sample ( table_ref , credentials , destination_table = None , size_bin = 500 , verbose = False ) \u00b6 Return sample of seed candidates. If no destination_table , output saved to table_ref (overwrite) Source code in techlandscape/candidates.py @app . command () def get_candidates_sample ( table_ref : str , credentials : Path , destination_table : str = None , size_bin : int = 500 , verbose : bool = False , ): \"\"\"Return sample of seed candidates. If no `destination_table`, output saved to `table_ref`(overwrite)\"\"\" query = f \"\"\" WITH table_stats AS ( SELECT *, SUM(nb_bin) OVER() AS nb_total FROM ( SELECT match, COUNT(match) AS nb_bin FROM ` { table_ref } ` GROUP BY match) ) SELECT * FROM ` { table_ref } ` JOIN table_stats USING (match) WHERE RAND()< { size_bin } /nb_bin ORDER BY RAND()\"\"\" destination_table = destination_table if destination_table else table_ref get_bq_job_done ( query , destination_table , credentials , verbose = verbose ) prep_prodigy_annotation ( data , config ) \u00b6 Return data with options for prodigy annotation to stdout. Nb: data is expected to be a jsonl file. Source code in techlandscape/candidates.py @app . command () def prep_prodigy_annotation ( data : Path , config : Path ): \"\"\"Return `data` with options for prodigy annotation to stdout. Nb: data is expected to be a jsonl file.\"\"\" def add_prodigy_options ( line : dict , options : List [ str ]): line . update ( { \"options\" : [{ \"id\" : i , \"text\" : option } for i , option in enumerate ( options )]} ) return line config = get_config ( config ) options = config . get ( \"option\" ) for line in Path ( data ) . open ( \"r\" ): line = json . loads ( line ) if options : line = add_prodigy_options ( line , options ) typer . echo ( json . dumps ( line ))","title":"candidates"},{"location":"API_CANDIDATES/#techlandscape.candidates.QueryCandidates","text":"Class","title":"QueryCandidates"},{"location":"API_CANDIDATES/#techlandscape.candidates.QueryCandidates.get_query","text":"Return candidates query (all) Source code in techlandscape/candidates.py def get_query ( self ) -> str : \"\"\"Return candidates query (all)\"\"\" query_patents = self . get_query_patents ( self . patents ) query_cpcs = self . get_query_cpcs ( self . cpcs ) query_keywords = self . get_query_keywords ( self . keywords ) query = f \"\"\" WITH tmp AS ( { \"UNION ALL\" . join ([ query_patents , query_cpcs , query_keywords ]) } ) SELECT p.family_id, STRING_AGG(tmp.publication_number) AS publication_number, CONCAT(ANY_VALUE(tmp.title), \" \\\\ n \\\\ n\", ANY_VALUE(tmp.abstract)) AS text, STRING_AGG(DISTINCT(tmp.match), \",\" order by tmp.match ASC ) AS match, ARRAY_LENGTH(SPLIT(STRING_AGG(tmp.match))) AS match_number, FROM tmp LEFT JOIN `patents-public-data.patents.publications` AS p ON tmp.publication_number=p.publication_number GROUP BY family_id \"\"\" return query","title":"get_query()"},{"location":"API_CANDIDATES/#techlandscape.candidates.QueryCandidates.get_query_cpcs","text":"Return candidates query based on cpcs Source code in techlandscape/candidates.py def get_query_cpcs ( self , cpcs : List [ str ]) -> str : \"\"\"Return candidates query based on cpcs\"\"\" cpcs = cpcs if cpcs else self . cpcs query = f \"\"\" SELECT publication_number, title, abstract, \"cpc\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr, UNNEST(cpc) as cpc WHERE { \" OR \" . join ( map ( lambda x : 'cpc.code LIKE \"' + x + '%\"' , cpcs )) } \"\"\" return query","title":"get_query_cpcs()"},{"location":"API_CANDIDATES/#techlandscape.candidates.QueryCandidates.get_query_keywords","text":"Return candidates query based on keywords Source code in techlandscape/candidates.py def get_query_keywords ( self , keywords : List [ str ]) -> str : \"\"\"Return candidates query based on keywords\"\"\" keywords = keywords if keywords else self . keywords query = f \"\"\" SELECT publication_number, title, abstract, \"keyword\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr WHERE { \" OR \" . join ( map ( lambda x : 'LOWER(gpr.abstract) LIKE \"%' + x + '%\"' , keywords )) } \"\"\" return query","title":"get_query_keywords()"},{"location":"API_CANDIDATES/#techlandscape.candidates.QueryCandidates.get_query_patents","text":"Return candidates query based on patent similarity Source code in techlandscape/candidates.py def get_query_patents ( self , patents : List [ str ]) -> str : \"\"\"Return candidates query based on patent similarity\"\"\" patents = patents if patents else self . patents query = f \"\"\" WITH SIMILAR AS ( SELECT similar.publication_number, \"patent\" AS match FROM `patents-public-data.google_patents_research.publications` AS gpr, UNNEST(similar) AS similar WHERE gpr.publication_number IN ( { \",\" . join ( map ( lambda x : '\"' + x + '\"' , patents )) } ) ) SELECT SIMILAR.publication_number, title, abstract, SIMILAR.match FROM `patents-public-data.google_patents_research.publications` AS gpr, SIMILAR WHERE SIMILAR.publication_number=gpr.publication_number AND SIMILAR.publication_number IS NOT NULL \"\"\" return query","title":"get_query_patents()"},{"location":"API_CANDIDATES/#techlandscape.candidates.get_candidates","text":"Return seed candidates based on config . Candidate table is saved to destination_table Source code in techlandscape/candidates.py @app . command () def get_candidates ( config : Path , destination_table : str , credentials : Path , verbose : bool = False ): \"\"\" Return seed candidates based on `config`. Candidate table is saved to `destination_table` \"\"\" query = QueryCandidates ( config ) . get_query () get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"get_candidates()"},{"location":"API_CANDIDATES/#techlandscape.candidates.get_candidates_sample","text":"Return sample of seed candidates. If no destination_table , output saved to table_ref (overwrite) Source code in techlandscape/candidates.py @app . command () def get_candidates_sample ( table_ref : str , credentials : Path , destination_table : str = None , size_bin : int = 500 , verbose : bool = False , ): \"\"\"Return sample of seed candidates. If no `destination_table`, output saved to `table_ref`(overwrite)\"\"\" query = f \"\"\" WITH table_stats AS ( SELECT *, SUM(nb_bin) OVER() AS nb_total FROM ( SELECT match, COUNT(match) AS nb_bin FROM ` { table_ref } ` GROUP BY match) ) SELECT * FROM ` { table_ref } ` JOIN table_stats USING (match) WHERE RAND()< { size_bin } /nb_bin ORDER BY RAND()\"\"\" destination_table = destination_table if destination_table else table_ref get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"get_candidates_sample()"},{"location":"API_CANDIDATES/#techlandscape.candidates.prep_prodigy_annotation","text":"Return data with options for prodigy annotation to stdout. Nb: data is expected to be a jsonl file. Source code in techlandscape/candidates.py @app . command () def prep_prodigy_annotation ( data : Path , config : Path ): \"\"\"Return `data` with options for prodigy annotation to stdout. Nb: data is expected to be a jsonl file.\"\"\" def add_prodigy_options ( line : dict , options : List [ str ]): line . update ( { \"options\" : [{ \"id\" : i , \"text\" : option } for i , option in enumerate ( options )]} ) return line config = get_config ( config ) options = config . get ( \"option\" ) for line in Path ( data ) . open ( \"r\" ): line = json . loads ( line ) if options : line = add_prodigy_options ( line , options ) typer . echo ( json . dumps ( line ))","title":"prep_prodigy_annotation()"},{"location":"API_EXPANSION/","text":"get_citation_expansion ( primary_key , citation_kind , expansion_level , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Expand \"along\" the citation dimension, either backward or forward Parameters: Name Type Description Default primary_key PrimaryKey table primary key required citation_kind CitationKind kind of citations required expansion_level CitationExpansionLevel expansion level required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_citation_expansion ( primary_key : PrimaryKey , citation_kind : CitationKind , expansion_level : CitationExpansionLevel , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Expand \"along\" the citation dimension, either backward or forward Arguments: primary_key: table primary key citation_kind: kind of citations expansion_level: expansion level table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" dataset = ( \"google_patents_research\" if citation_kind . value == CitationKind . forward . value and primary_key . value == PrimaryKey . publication_number . value else \"patents\" ) expansion_level_clause = ( 'AND expansion_level in (\"SEED\", \"PC\")' if expansion_level . value == CitationExpansionLevel . L1 . value else 'AND expansion_level LIKE \"L1%\"' ) project_id_ = get_project_id ( primary_key , credentials ) # query_suffix = ( # \"\" # if primary_key.value == PrimaryKey.publication_number.value # else f\"\"\", UNNEST({primary_key.value}) as {primary_key.value}\"\"\" # ) cit_var = ( \"citation\" if citation_kind . value == CitationKind . backward . value else \"cited_by\" ) query = f \"\"\" WITH expansion AS( SELECT DISTINCT cit. { primary_key . value } FROM ` { project_id_ } . { dataset } .publications` AS p, { table_ref } AS tmp, UNNEST( { cit_var } ) AS cit WHERE p. { primary_key . value } =tmp. { primary_key . value } AND cit. { primary_key . value } IS NOT NULL { expansion_level_clause } ) SELECT DISTINCT( { primary_key . value } ), \" { '-' . join ([ expansion_level . value , citation_kind . value . upper ()]) } \" AS expansion_level FROM expansion \"\"\" # {query_suffix} get_bq_job_done ( query , destination_table , credentials , ** kwargs ) get_expansion ( primary_key , tech_class , table_ref , destination_table , staging_dataset , credentials , random_share = None , precomputed = False , n_pc = 50 ) \u00b6 Expand along PC and citations (L1 + L2) Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str seed table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required staging_dataset str intermediary table staging dataset (project.dataset) required credentials Path BQ credentials file path required random_share float share of the seed randomly drawn and used for the expansion None precomputed bool True if pc odds pre-computed False n_pc int nb most important pc for pc expansion 50 Usage: TECH = \"additivemanufacturing\" techlandscape expansion get-expansion family_id cpc patentcity.techdiffusion.seed_ ${ TECH } patentcity.techdiffusion.expansion_ ${ TECH } patentcity.stage credentials_bq.json Source code in techlandscape/expansion.py @app . command () def get_expansion ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , staging_dataset : str , credentials : Path , random_share : float = None , precomputed : bool = False , n_pc : int = 50 , ): \"\"\" Expand along PC and citations (L1 + L2) Arguments: primary_key: table primary key tech_class: technological class considered table_ref: seed table (project.dataset.table) destination_table: query results destination table (project.dataset.table) staging_dataset: intermediary table staging dataset (project.dataset) credentials: BQ credentials file path random_share: share of the seed randomly drawn and used for the expansion precomputed: True if pc odds pre-computed n_pc: nb most important pc for pc expansion **Usage:** ```shell TECH=\"additivemanufacturing\" techlandscape expansion get-expansion family_id cpc patentcity.techdiffusion.seed_${TECH} patentcity.techdiffusion.expansion_${TECH} patentcity.stage credentials_bq.json ``` \"\"\" get_seed ( primary_key , table_ref , destination_table , credentials , random_share , write_disposition = \"WRITE_TRUNCATE\" , verbose = False , ) get_full_pc_expansion ( primary_key , tech_class , n_pc , destination_table , destination_table , staging_dataset , credentials , precomputed , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_full_citation_expansion ( primary_key , CitationExpansionLevel . L1 , destination_table , destination_table , credentials , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_full_citation_expansion ( primary_key , CitationExpansionLevel . L2 , destination_table , destination_table , credentials , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_reduced_expansion ( primary_key , destination_table , destination_table , credentials , write_disposition = \"WRITE_TRUNCATE\" , verbose = False , ) get_full_citation_expansion ( primary_key , expansion_level , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Expand along the citation level, both backward and forward Parameters: Name Type Description Default primary_key PrimaryKey table primary key required expansion_level CitationExpansionLevel expansion level required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_full_citation_expansion ( primary_key : PrimaryKey , expansion_level : CitationExpansionLevel , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\"Expand along the citation level, both backward and forward Arguments: primary_key: table primary key expansion_level: expansion level table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" get_citation_expansion ( primary_key , CitationKind . backward , expansion_level , table_ref , destination_table , credentials , ** kwargs , ) get_citation_expansion ( primary_key , CitationKind . forward , expansion_level , table_ref , destination_table , credentials , ** kwargs , ) get_full_pc_expansion ( primary_key , tech_class , n_pc , table_ref , destination_table , staging_dataset , credentials , precomputed = False , ** kwargs ) \u00b6 Compute (or use precomputed) seed pc odds and expand along the pc dimension. Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required n_pc int nb most important pc for pc expansion required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required staging_dataset str intermediary table staging dataset (project.dataset) required credentials Path BQ credentials file path required precomputed bool True if pc odds pre-computed False **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_full_pc_expansion ( primary_key : PrimaryKey , tech_class : TechClass , n_pc : int , table_ref : str , destination_table : str , staging_dataset : str , credentials : Path , precomputed : bool = False , ** kwargs , ): \"\"\"Compute (or use precomputed) seed pc odds and expand along the pc dimension. Arguments: primary_key: table primary key tech_class: technological class considered n_pc: nb most important pc for pc expansion table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) staging_dataset: intermediary table staging dataset (project.dataset) credentials: BQ credentials file path precomputed: True if pc odds pre-computed **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" name = table_ref . split ( \".\" )[ - 1 ] . replace ( \"seed_\" , \"\" ) if not precomputed : get_seed_pc_freq ( primary_key , tech_class , table_ref , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _freq\" , credentials , verbose = False , ) get_universe_pc_freq ( primary_key , tech_class , table_ref , f \" { staging_dataset } . { name } _universe_ { tech_class . value } _freq\" , credentials , verbose = False , ) get_seed_pc_odds ( tech_class , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _freq\" , f \" { staging_dataset } . { name } _universe_ { tech_class . value } _freq\" , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _odds\" , credentials , verbose = False , ) get_pc_expansion ( primary_key , tech_class , n_pc , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _odds\" , destination_table , credentials , ** kwargs , ) get_pc_expansion ( primary_key , tech_class , n_pc , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Expand the seed \"along\" the pc dimension for patent classes with large odds in Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required n_pc int nb most important pc for pc expansion required table_ref str pc odds table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_pc_expansion ( primary_key : PrimaryKey , tech_class : TechClass , n_pc : int , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Expand the seed \"along\" the pc dimension for patent classes with large odds in Arguments: primary_key: table primary key tech_class: technological class considered n_pc: nb most important pc for pc expansion table_ref: pc odds table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH important_pc AS ( SELECT { tech_class . value } FROM ` { table_ref } ` ORDER BY odds DESC LIMIT { n_pc } ), patents AS ( SELECT { primary_key . value } , { tech_class . value } .code AS { tech_class . value } FROM ` { project_id } .patents.publications`, UNNEST( { tech_class . value } ) AS { tech_class . value } ) SELECT patents. { primary_key . value } , \"PC\" as expansion_level # STRING_AGG(DISTINCT(patents. { tech_class . value } )) AS { tech_class . value } ## dbg FROM important_pc LEFT JOIN patents ON important_pc. { tech_class . value } =patents. { tech_class . value } GROUP BY { primary_key . value } \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs ) get_reduced_expansion ( primary_key , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Return the table_ref in a reduced format. Each primary_key appears only once primary_key : table primary key table_ref : expansion table ( project . dataset . table ) destination_table : query results destination table ( project . dataset . table ) credentials : BQ credentials file path ** kwargs : key worded args passed to bigquery . QueryJobConfig Usage: Source code in techlandscape/expansion.py def get_reduced_expansion ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the `table_ref` in a reduced format. Each `primary_key` appears only once primary_key: table primary key table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" query = f \"\"\" SELECT { primary_key . value } , CASE WHEN STRING_AGG(expansion_level) LIKE \"%ANTISEED%\" THEN \"ANTISEED\" WHEN STRING_AGG(expansion_level) LIKE \"%SEED%\" THEN \"SEED\" WHEN STRING_AGG(expansion_level) LIKE \"%PC%\" THEN \"PC\" WHEN STRING_AGG(expansion_level) LIKE \"%L1-BACK%\" THEN \"L1-BACK\" WHEN STRING_AGG(expansion_level) LIKE \"%L1-FOR%\" THEN \"L1-FOR\" WHEN STRING_AGG(expansion_level) LIKE \"%L2-BACK%\" THEN \"L2-BACK\" WHEN STRING_AGG(expansion_level) LIKE \"%L2-FOR%\" THEN \"L2-FOR\" ELSE NULL END AS expansion_level, STRING_AGG(expansion_level) as expansion_level_, COUNT(expansion_level) as nb_match, FROM ` { table_ref } ` GROUP BY { primary_key . value } \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs ) get_seed ( primary_key , table_ref , destination_table , credentials , random_share = None , verbose = False , ** kwargs ) \u00b6 Return the starting block of the expansion. Random draw enabled (e.g. for robustness analysis) Parameters: Name Type Description Default primary_key PrimaryKey table primary key required table_ref str manually annotated seed table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required random_share float size of the random draw (if not None) None verbose bool verbosity False **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_seed ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , random_share : float = None , verbose : bool = False , ** kwargs , ): \"\"\" Return the starting block of the expansion. Random draw enabled (e.g. for robustness analysis) Arguments: primary_key: table primary key table_ref: manually annotated seed table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path random_share: size of the random draw (if not None) verbose: verbosity **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" random_share_clause = f \"\"\"AND RAND()< { random_share } \"\"\" if random_share else \"\" query = f \"\"\" SELECT { primary_key . value } , expansion_level FROM ` { table_ref } ` # patentcity.techdiffusion.seed_additivemanufacturing WHERE expansion_level LIKE \"%SEED%\" { random_share_clause } \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose , ** kwargs ) get_seed_pc_freq ( primary_key , tech_class , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Return the frequency of patent classes in the seed Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_seed_pc_freq ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the frequency of patent classes in the seed Arguments: primary_key: table primary key tech_class: technological class considered table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH tmp AS ( SELECT tmp. { primary_key . value } , { tech_class . value } .code AS { tech_class . value } FROM ` { project_id } .patents.publications` AS p, ` { table_ref } ` AS tmp, UNNEST( { tech_class . value } ) AS { tech_class . value } WHERE p. { primary_key . value } =tmp. { primary_key . value } and tmp.expansion_level = \"SEED\" ) , total AS ( SELECT COUNT( { primary_key . value } ) as count FROM ` { table_ref } ` WHERE expansion_level = \"SEED\" ) SELECT { tech_class . value } , count( { tech_class . value } ) as n_ { tech_class . value } , total.count as n_patents, count( { tech_class . value } )/total.count as freq FROM tmp, total GROUP BY { tech_class . value } , total.count ORDER BY freq DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs ) get_seed_pc_odds ( tech_class , table_seed , table_universe , destination_table , credentials , ** kwargs ) \u00b6 Return the odds of patent classes in seed compared to the universe of patents Parameters: Name Type Description Default tech_class TechClass technological class considered required table_seed str pc freq seed table (project.dataset.table) required table_universe str pc freq universe table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Source code in techlandscape/expansion.py def get_seed_pc_odds ( tech_class : TechClass , table_seed : str , table_universe : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the odds of patent classes in seed compared to the universe of patents Arguments: tech_class: technological class considered table_seed: pc freq seed table (project.dataset.table) table_universe: pc freq universe table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig \"\"\" query = f \"\"\" WITH tmp AS ( SELECT seed. { tech_class . value } as { tech_class . value } , seed.freq as seed_freq, universe.freq as universe_freq FROM ` { table_seed } ` as seed, ` { table_universe } ` as universe WHERE seed. { tech_class . value } = universe. { tech_class . value } and seed. { tech_class . value } IS NOT NULL) SELECT { tech_class . value } , seed_freq, universe_freq, seed_freq / universe_freq as odds FROM tmp ORDER BY odds DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs ) get_universe_pc_freq ( primary_key , tech_class , table_ref , destination_table , credentials , ** kwargs ) \u00b6 Return the frequency of patent classes in the universe of patents. Nb: we restrict to patents published between p25 and p75 of the publication_year of patents in the seed. Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_universe_pc_freq ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the frequency of patent classes in the universe of patents. Nb: we restrict to patents published between p25 and p75 of the publication_year of patents in the seed. Arguments: primary_key: table primary key tech_class: technological class considered table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" # TODO unrestricted time span option? # Support family_id level? project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH tmp AS ( SELECT tmp. { primary_key . value } , CAST(p.publication_date/10000 AS INT64) AS publication_year FROM { table_ref } AS tmp, ` { project_id } .patents.publications` AS p WHERE p. { primary_key . value } =tmp. { primary_key . value } AND tmp.expansion_level=\"SEED\"), stats AS ( SELECT percentiles[OFFSET(25)] AS p25, percentiles[OFFSET(75)] AS p75 FROM ( SELECT APPROX_QUANTILES(publication_year, 100) AS percentiles FROM tmp)), total as (SELECT count( { primary_key . value } ) as count FROM ` { project_id } .patents.publications` AS p, stats WHERE publication_date BETWEEN stats.p25*10000 AND stats.p75*10000) SELECT { tech_class . value } .code AS { tech_class . value } , COUNT( { tech_class . value } .code) as n_cpc, total.count as n_patents, COUNT( { tech_class . value } .code)/total.count as freq FROM ` { project_id } .patents.publications` AS p, UNNEST( { tech_class . value } ) AS { tech_class . value } , stats, total WHERE publication_date BETWEEN stats.p25*10000 AND stats.p75*10000 GROUP BY { tech_class . value } , total.count ORDER BY freq DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"expansion"},{"location":"API_EXPANSION/#techlandscape.expansion.get_citation_expansion","text":"Expand \"along\" the citation dimension, either backward or forward Parameters: Name Type Description Default primary_key PrimaryKey table primary key required citation_kind CitationKind kind of citations required expansion_level CitationExpansionLevel expansion level required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_citation_expansion ( primary_key : PrimaryKey , citation_kind : CitationKind , expansion_level : CitationExpansionLevel , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Expand \"along\" the citation dimension, either backward or forward Arguments: primary_key: table primary key citation_kind: kind of citations expansion_level: expansion level table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" dataset = ( \"google_patents_research\" if citation_kind . value == CitationKind . forward . value and primary_key . value == PrimaryKey . publication_number . value else \"patents\" ) expansion_level_clause = ( 'AND expansion_level in (\"SEED\", \"PC\")' if expansion_level . value == CitationExpansionLevel . L1 . value else 'AND expansion_level LIKE \"L1%\"' ) project_id_ = get_project_id ( primary_key , credentials ) # query_suffix = ( # \"\" # if primary_key.value == PrimaryKey.publication_number.value # else f\"\"\", UNNEST({primary_key.value}) as {primary_key.value}\"\"\" # ) cit_var = ( \"citation\" if citation_kind . value == CitationKind . backward . value else \"cited_by\" ) query = f \"\"\" WITH expansion AS( SELECT DISTINCT cit. { primary_key . value } FROM ` { project_id_ } . { dataset } .publications` AS p, { table_ref } AS tmp, UNNEST( { cit_var } ) AS cit WHERE p. { primary_key . value } =tmp. { primary_key . value } AND cit. { primary_key . value } IS NOT NULL { expansion_level_clause } ) SELECT DISTINCT( { primary_key . value } ), \" { '-' . join ([ expansion_level . value , citation_kind . value . upper ()]) } \" AS expansion_level FROM expansion \"\"\" # {query_suffix} get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_citation_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_expansion","text":"Expand along PC and citations (L1 + L2) Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str seed table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required staging_dataset str intermediary table staging dataset (project.dataset) required credentials Path BQ credentials file path required random_share float share of the seed randomly drawn and used for the expansion None precomputed bool True if pc odds pre-computed False n_pc int nb most important pc for pc expansion 50 Usage: TECH = \"additivemanufacturing\" techlandscape expansion get-expansion family_id cpc patentcity.techdiffusion.seed_ ${ TECH } patentcity.techdiffusion.expansion_ ${ TECH } patentcity.stage credentials_bq.json Source code in techlandscape/expansion.py @app . command () def get_expansion ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , staging_dataset : str , credentials : Path , random_share : float = None , precomputed : bool = False , n_pc : int = 50 , ): \"\"\" Expand along PC and citations (L1 + L2) Arguments: primary_key: table primary key tech_class: technological class considered table_ref: seed table (project.dataset.table) destination_table: query results destination table (project.dataset.table) staging_dataset: intermediary table staging dataset (project.dataset) credentials: BQ credentials file path random_share: share of the seed randomly drawn and used for the expansion precomputed: True if pc odds pre-computed n_pc: nb most important pc for pc expansion **Usage:** ```shell TECH=\"additivemanufacturing\" techlandscape expansion get-expansion family_id cpc patentcity.techdiffusion.seed_${TECH} patentcity.techdiffusion.expansion_${TECH} patentcity.stage credentials_bq.json ``` \"\"\" get_seed ( primary_key , table_ref , destination_table , credentials , random_share , write_disposition = \"WRITE_TRUNCATE\" , verbose = False , ) get_full_pc_expansion ( primary_key , tech_class , n_pc , destination_table , destination_table , staging_dataset , credentials , precomputed , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_full_citation_expansion ( primary_key , CitationExpansionLevel . L1 , destination_table , destination_table , credentials , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_full_citation_expansion ( primary_key , CitationExpansionLevel . L2 , destination_table , destination_table , credentials , write_disposition = \"WRITE_APPEND\" , verbose = False , ) get_reduced_expansion ( primary_key , destination_table , destination_table , credentials , write_disposition = \"WRITE_TRUNCATE\" , verbose = False , )","title":"get_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_full_citation_expansion","text":"Expand along the citation level, both backward and forward Parameters: Name Type Description Default primary_key PrimaryKey table primary key required expansion_level CitationExpansionLevel expansion level required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_full_citation_expansion ( primary_key : PrimaryKey , expansion_level : CitationExpansionLevel , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\"Expand along the citation level, both backward and forward Arguments: primary_key: table primary key expansion_level: expansion level table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" get_citation_expansion ( primary_key , CitationKind . backward , expansion_level , table_ref , destination_table , credentials , ** kwargs , ) get_citation_expansion ( primary_key , CitationKind . forward , expansion_level , table_ref , destination_table , credentials , ** kwargs , )","title":"get_full_citation_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_full_pc_expansion","text":"Compute (or use precomputed) seed pc odds and expand along the pc dimension. Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required n_pc int nb most important pc for pc expansion required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required staging_dataset str intermediary table staging dataset (project.dataset) required credentials Path BQ credentials file path required precomputed bool True if pc odds pre-computed False **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_full_pc_expansion ( primary_key : PrimaryKey , tech_class : TechClass , n_pc : int , table_ref : str , destination_table : str , staging_dataset : str , credentials : Path , precomputed : bool = False , ** kwargs , ): \"\"\"Compute (or use precomputed) seed pc odds and expand along the pc dimension. Arguments: primary_key: table primary key tech_class: technological class considered n_pc: nb most important pc for pc expansion table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) staging_dataset: intermediary table staging dataset (project.dataset) credentials: BQ credentials file path precomputed: True if pc odds pre-computed **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" name = table_ref . split ( \".\" )[ - 1 ] . replace ( \"seed_\" , \"\" ) if not precomputed : get_seed_pc_freq ( primary_key , tech_class , table_ref , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _freq\" , credentials , verbose = False , ) get_universe_pc_freq ( primary_key , tech_class , table_ref , f \" { staging_dataset } . { name } _universe_ { tech_class . value } _freq\" , credentials , verbose = False , ) get_seed_pc_odds ( tech_class , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _freq\" , f \" { staging_dataset } . { name } _universe_ { tech_class . value } _freq\" , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _odds\" , credentials , verbose = False , ) get_pc_expansion ( primary_key , tech_class , n_pc , f \" { staging_dataset } . { name } _seed_ { tech_class . value } _odds\" , destination_table , credentials , ** kwargs , )","title":"get_full_pc_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_pc_expansion","text":"Expand the seed \"along\" the pc dimension for patent classes with large odds in Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required n_pc int nb most important pc for pc expansion required table_ref str pc odds table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_pc_expansion ( primary_key : PrimaryKey , tech_class : TechClass , n_pc : int , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Expand the seed \"along\" the pc dimension for patent classes with large odds in Arguments: primary_key: table primary key tech_class: technological class considered n_pc: nb most important pc for pc expansion table_ref: pc odds table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH important_pc AS ( SELECT { tech_class . value } FROM ` { table_ref } ` ORDER BY odds DESC LIMIT { n_pc } ), patents AS ( SELECT { primary_key . value } , { tech_class . value } .code AS { tech_class . value } FROM ` { project_id } .patents.publications`, UNNEST( { tech_class . value } ) AS { tech_class . value } ) SELECT patents. { primary_key . value } , \"PC\" as expansion_level # STRING_AGG(DISTINCT(patents. { tech_class . value } )) AS { tech_class . value } ## dbg FROM important_pc LEFT JOIN patents ON important_pc. { tech_class . value } =patents. { tech_class . value } GROUP BY { primary_key . value } \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_pc_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_reduced_expansion","text":"Return the table_ref in a reduced format. Each primary_key appears only once primary_key : table primary key table_ref : expansion table ( project . dataset . table ) destination_table : query results destination table ( project . dataset . table ) credentials : BQ credentials file path ** kwargs : key worded args passed to bigquery . QueryJobConfig Usage: Source code in techlandscape/expansion.py def get_reduced_expansion ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the `table_ref` in a reduced format. Each `primary_key` appears only once primary_key: table primary key table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" query = f \"\"\" SELECT { primary_key . value } , CASE WHEN STRING_AGG(expansion_level) LIKE \"%ANTISEED%\" THEN \"ANTISEED\" WHEN STRING_AGG(expansion_level) LIKE \"%SEED%\" THEN \"SEED\" WHEN STRING_AGG(expansion_level) LIKE \"%PC%\" THEN \"PC\" WHEN STRING_AGG(expansion_level) LIKE \"%L1-BACK%\" THEN \"L1-BACK\" WHEN STRING_AGG(expansion_level) LIKE \"%L1-FOR%\" THEN \"L1-FOR\" WHEN STRING_AGG(expansion_level) LIKE \"%L2-BACK%\" THEN \"L2-BACK\" WHEN STRING_AGG(expansion_level) LIKE \"%L2-FOR%\" THEN \"L2-FOR\" ELSE NULL END AS expansion_level, STRING_AGG(expansion_level) as expansion_level_, COUNT(expansion_level) as nb_match, FROM ` { table_ref } ` GROUP BY { primary_key . value } \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_reduced_expansion()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_seed","text":"Return the starting block of the expansion. Random draw enabled (e.g. for robustness analysis) Parameters: Name Type Description Default primary_key PrimaryKey table primary key required table_ref str manually annotated seed table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required random_share float size of the random draw (if not None) None verbose bool verbosity False **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_seed ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , random_share : float = None , verbose : bool = False , ** kwargs , ): \"\"\" Return the starting block of the expansion. Random draw enabled (e.g. for robustness analysis) Arguments: primary_key: table primary key table_ref: manually annotated seed table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path random_share: size of the random draw (if not None) verbose: verbosity **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" random_share_clause = f \"\"\"AND RAND()< { random_share } \"\"\" if random_share else \"\" query = f \"\"\" SELECT { primary_key . value } , expansion_level FROM ` { table_ref } ` # patentcity.techdiffusion.seed_additivemanufacturing WHERE expansion_level LIKE \"%SEED%\" { random_share_clause } \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose , ** kwargs )","title":"get_seed()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_seed_pc_freq","text":"Return the frequency of patent classes in the seed Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_seed_pc_freq ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the frequency of patent classes in the seed Arguments: primary_key: table primary key tech_class: technological class considered table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH tmp AS ( SELECT tmp. { primary_key . value } , { tech_class . value } .code AS { tech_class . value } FROM ` { project_id } .patents.publications` AS p, ` { table_ref } ` AS tmp, UNNEST( { tech_class . value } ) AS { tech_class . value } WHERE p. { primary_key . value } =tmp. { primary_key . value } and tmp.expansion_level = \"SEED\" ) , total AS ( SELECT COUNT( { primary_key . value } ) as count FROM ` { table_ref } ` WHERE expansion_level = \"SEED\" ) SELECT { tech_class . value } , count( { tech_class . value } ) as n_ { tech_class . value } , total.count as n_patents, count( { tech_class . value } )/total.count as freq FROM tmp, total GROUP BY { tech_class . value } , total.count ORDER BY freq DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_seed_pc_freq()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_seed_pc_odds","text":"Return the odds of patent classes in seed compared to the universe of patents Parameters: Name Type Description Default tech_class TechClass technological class considered required table_seed str pc freq seed table (project.dataset.table) required table_universe str pc freq universe table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Source code in techlandscape/expansion.py def get_seed_pc_odds ( tech_class : TechClass , table_seed : str , table_universe : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the odds of patent classes in seed compared to the universe of patents Arguments: tech_class: technological class considered table_seed: pc freq seed table (project.dataset.table) table_universe: pc freq universe table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig \"\"\" query = f \"\"\" WITH tmp AS ( SELECT seed. { tech_class . value } as { tech_class . value } , seed.freq as seed_freq, universe.freq as universe_freq FROM ` { table_seed } ` as seed, ` { table_universe } ` as universe WHERE seed. { tech_class . value } = universe. { tech_class . value } and seed. { tech_class . value } IS NOT NULL) SELECT { tech_class . value } , seed_freq, universe_freq, seed_freq / universe_freq as odds FROM tmp ORDER BY odds DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_seed_pc_odds()"},{"location":"API_EXPANSION/#techlandscape.expansion.get_universe_pc_freq","text":"Return the frequency of patent classes in the universe of patents. Nb: we restrict to patents published between p25 and p75 of the publication_year of patents in the seed. Parameters: Name Type Description Default primary_key PrimaryKey table primary key required tech_class TechClass technological class considered required table_ref str expansion table (project.dataset.table) required destination_table str query results destination table (project.dataset.table) required credentials Path BQ credentials file path required **kwargs key worded args passed to bigquery.QueryJobConfig {} Usage: Source code in techlandscape/expansion.py def get_universe_pc_freq ( primary_key : PrimaryKey , tech_class : TechClass , table_ref : str , destination_table : str , credentials : Path , ** kwargs , ): \"\"\" Return the frequency of patent classes in the universe of patents. Nb: we restrict to patents published between p25 and p75 of the publication_year of patents in the seed. Arguments: primary_key: table primary key tech_class: technological class considered table_ref: expansion table (project.dataset.table) destination_table: query results destination table (project.dataset.table) credentials: BQ credentials file path **kwargs: key worded args passed to bigquery.QueryJobConfig **Usage:** \"\"\" # TODO unrestricted time span option? # Support family_id level? project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH tmp AS ( SELECT tmp. { primary_key . value } , CAST(p.publication_date/10000 AS INT64) AS publication_year FROM { table_ref } AS tmp, ` { project_id } .patents.publications` AS p WHERE p. { primary_key . value } =tmp. { primary_key . value } AND tmp.expansion_level=\"SEED\"), stats AS ( SELECT percentiles[OFFSET(25)] AS p25, percentiles[OFFSET(75)] AS p75 FROM ( SELECT APPROX_QUANTILES(publication_year, 100) AS percentiles FROM tmp)), total as (SELECT count( { primary_key . value } ) as count FROM ` { project_id } .patents.publications` AS p, stats WHERE publication_date BETWEEN stats.p25*10000 AND stats.p75*10000) SELECT { tech_class . value } .code AS { tech_class . value } , COUNT( { tech_class . value } .code) as n_cpc, total.count as n_patents, COUNT( { tech_class . value } .code)/total.count as freq FROM ` { project_id } .patents.publications` AS p, UNNEST( { tech_class . value } ) AS { tech_class . value } , stats, total WHERE publication_date BETWEEN stats.p25*10000 AND stats.p75*10000 GROUP BY { tech_class . value } , total.count ORDER BY freq DESC \"\"\" get_bq_job_done ( query , destination_table , credentials , ** kwargs )","title":"get_universe_pc_freq()"},{"location":"API_IO/","text":"get_expansion ( primary_key , table_ref , destination_table , credentials , sample_size = None , verbose = False ) \u00b6 Return (a sample of) the expansion table Parameters: Name Type Description Default primary_key PrimaryKey table primary key required table_ref str expansion table required destination_table str destination table required credentials Path credentials file path required sample_size int size of the sample (if None, then we extract all) None verbose bool verbosity False Usage: techlandscape io get-expansion family_id <table-ref> <destination-table> credentials_bq.json --sample-size 10000 Source code in techlandscape/io.py @monitor @app . command () def get_expansion ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , sample_size : int = None , verbose : bool = False , ) -> None : \"\"\" Return (a sample of) the expansion table Arguments: primary_key: table primary key table_ref: expansion table destination_table: destination table credentials: credentials file path sample_size: size of the sample (if None, then we extract all) verbose: verbosity **Usage:** ``` techlandscape io get-expansion family_id <table-ref> <destination-table> credentials_bq.json --sample-size 10000 ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) sample_clause = f \"LIMIT { sample_size } \" if sample_size else \"\" query = f \"\"\" WITH expansion AS ( SELECT * FROM ` { table_ref } ` AS expansion # patentcity.techdiffusion.expansion_additivemanufacturing WHERE expansion_level NOT LIKE \"%SEED%\" ) SELECT expansion.*, p.abstract AS text FROM expansion LEFT JOIN ` { project_id } .patents.publications` AS p ON expansion. { primary_key . value } = p. { primary_key . value } WHERE p.abstract IS NOT NULL AND p.abstract != \"\" AND p.abstract != \" \\\\ n\" AND p.abstract != \" \\\\ n \\\\ n\" ORDER BY RAND() { sample_clause } \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose ) get_training_data ( primary_key , seed_table , expansion_table , af_antiseed_size , destination_table , credentials , verbose = False ) \u00b6 Return training data Parameters: Name Type Description Default primary_key PrimaryKey table primary key required seed_table str seed table (project.dataset.table) required expansion_table str expansion table used for drawing the antiseed a la AF (project.dataset.table) required af_antiseed_size int size of the antiseed a la AF required destination_table str destination table required credentials Path BQ credentials file path required verbose bool verbosity False Usage: techlandscape io get-training-data family_id <seed-table> <expansion-table> <af-seed-size> <destination-table> credentials_bq.json Source code in techlandscape/io.py @app . command () def get_training_data ( primary_key : PrimaryKey , seed_table : str , expansion_table : str , af_antiseed_size : int , destination_table : str , credentials : Path , verbose : bool = False , ) -> None : \"\"\" Return training data Arguments: primary_key: table primary key seed_table: seed table (project.dataset.table) expansion_table: expansion table used for drawing the antiseed a la AF (project.dataset.table) af_antiseed_size: size of the antiseed a la AF destination_table: destination table credentials: BQ credentials file path verbose: verbosity **Usage:** ```shell techlandscape io get-training-data family_id <seed-table> <expansion-table> <af-seed-size> <destination-table> credentials_bq.json ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH manual AS ( SELECT { primary_key . value } , expansion_level, CAST(expansion_level=\"SEED\" AS INT64) AS is_seed, STRUCT(CAST(expansion_level=\"SEED\" AS INT64) AS SEED, CAST(expansion_level LIKE \"ANTISEED%\" AS INT64) AS NOT_SEED) AS cats, text FROM ` { seed_table } ` WHERE expansion_level LIKE \"%SEED%\"), random AS ( SELECT r. { primary_key . value } AS { primary_key . value } , \"ANTISEED-AF\" AS expansion_level, CAST(0 AS INT64) AS is_seed, STRUCT(0 AS SEED, 1 AS NOT_SEED) AS cats, abstract AS text FROM ` { project_id } .patents.publications` AS r #country_prefix LEFT OUTER JOIN ` { expansion_table } ` AS tmp ON r. { primary_key . value } = tmp. { primary_key . value } WHERE abstract IS NOT NULL AND abstract != \"\" ORDER BY RAND() LIMIT { af_antiseed_size } ) SELECT * FROM manual UNION ALL SELECT * FROM random \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"io"},{"location":"API_IO/#techlandscape.io.get_expansion","text":"Return (a sample of) the expansion table Parameters: Name Type Description Default primary_key PrimaryKey table primary key required table_ref str expansion table required destination_table str destination table required credentials Path credentials file path required sample_size int size of the sample (if None, then we extract all) None verbose bool verbosity False Usage: techlandscape io get-expansion family_id <table-ref> <destination-table> credentials_bq.json --sample-size 10000 Source code in techlandscape/io.py @monitor @app . command () def get_expansion ( primary_key : PrimaryKey , table_ref : str , destination_table : str , credentials : Path , sample_size : int = None , verbose : bool = False , ) -> None : \"\"\" Return (a sample of) the expansion table Arguments: primary_key: table primary key table_ref: expansion table destination_table: destination table credentials: credentials file path sample_size: size of the sample (if None, then we extract all) verbose: verbosity **Usage:** ``` techlandscape io get-expansion family_id <table-ref> <destination-table> credentials_bq.json --sample-size 10000 ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) sample_clause = f \"LIMIT { sample_size } \" if sample_size else \"\" query = f \"\"\" WITH expansion AS ( SELECT * FROM ` { table_ref } ` AS expansion # patentcity.techdiffusion.expansion_additivemanufacturing WHERE expansion_level NOT LIKE \"%SEED%\" ) SELECT expansion.*, p.abstract AS text FROM expansion LEFT JOIN ` { project_id } .patents.publications` AS p ON expansion. { primary_key . value } = p. { primary_key . value } WHERE p.abstract IS NOT NULL AND p.abstract != \"\" AND p.abstract != \" \\\\ n\" AND p.abstract != \" \\\\ n \\\\ n\" ORDER BY RAND() { sample_clause } \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"get_expansion()"},{"location":"API_IO/#techlandscape.io.get_training_data","text":"Return training data Parameters: Name Type Description Default primary_key PrimaryKey table primary key required seed_table str seed table (project.dataset.table) required expansion_table str expansion table used for drawing the antiseed a la AF (project.dataset.table) required af_antiseed_size int size of the antiseed a la AF required destination_table str destination table required credentials Path BQ credentials file path required verbose bool verbosity False Usage: techlandscape io get-training-data family_id <seed-table> <expansion-table> <af-seed-size> <destination-table> credentials_bq.json Source code in techlandscape/io.py @app . command () def get_training_data ( primary_key : PrimaryKey , seed_table : str , expansion_table : str , af_antiseed_size : int , destination_table : str , credentials : Path , verbose : bool = False , ) -> None : \"\"\" Return training data Arguments: primary_key: table primary key seed_table: seed table (project.dataset.table) expansion_table: expansion table used for drawing the antiseed a la AF (project.dataset.table) af_antiseed_size: size of the antiseed a la AF destination_table: destination table credentials: BQ credentials file path verbose: verbosity **Usage:** ```shell techlandscape io get-training-data family_id <seed-table> <expansion-table> <af-seed-size> <destination-table> credentials_bq.json ``` \"\"\" project_id = get_project_id ( primary_key , credentials ) query = f \"\"\" WITH manual AS ( SELECT { primary_key . value } , expansion_level, CAST(expansion_level=\"SEED\" AS INT64) AS is_seed, STRUCT(CAST(expansion_level=\"SEED\" AS INT64) AS SEED, CAST(expansion_level LIKE \"ANTISEED%\" AS INT64) AS NOT_SEED) AS cats, text FROM ` { seed_table } ` WHERE expansion_level LIKE \"%SEED%\"), random AS ( SELECT r. { primary_key . value } AS { primary_key . value } , \"ANTISEED-AF\" AS expansion_level, CAST(0 AS INT64) AS is_seed, STRUCT(0 AS SEED, 1 AS NOT_SEED) AS cats, abstract AS text FROM ` { project_id } .patents.publications` AS r #country_prefix LEFT OUTER JOIN ` { expansion_table } ` AS tmp ON r. { primary_key . value } = tmp. { primary_key . value } WHERE abstract IS NOT NULL AND abstract != \"\" ORDER BY RAND() LIMIT { af_antiseed_size } ) SELECT * FROM manual UNION ALL SELECT * FROM random \"\"\" get_bq_job_done ( query , destination_table , credentials , verbose = verbose )","title":"get_training_data()"},{"location":"API_MODEL/","text":"DataLoader \u00b6 Load data (train, test) Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import DataLoader from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) data_loader = DataLoader ( cfg ) data_loader . load () # check examples data_loader . text_train load ( self ) \u00b6 Load data. Expect a jsonl file where each row at least two fields: 'text' and 'is_seed'. Source code in techlandscape/model.py def load ( self ): \"\"\"Load data. Expect a jsonl file where each row at least two fields: 'text' and 'is_seed'.\"\"\" if any ( map ( lambda x : x is None , [ self . text_train , self . text_test , self . y_train , self . y_test ], ) ): self . text_train = self . _get_data ( self . train_path , \"text\" ) self . text_test = self . _get_data ( self . test_path , \"text\" ) self . y_train = np . array ( self . _get_data ( self . train_path , \"is_seed\" )) . astype ( int ) try : self . y_test = np . array ( self . _get_data ( self . test_path , \"is_seed\" ) ) . astype ( int ) except KeyError : typer . secho ( \"No output variable in test. You can still vectorize the data.\" , color = typer . colors . YELLOW , ) pass typer . secho ( f \" { ok } Data loaded\" , color = typer . colors . GREEN ) else : typer . secho ( \"Data already populated\" , color = typer . colors . YELLOW ) typer . secho ( f \" { ok }{ len ( self . text_train ) } examples loaded in training set\" , color = typer . colors . BLUE , ) typer . secho ( f \" { ok }{ len ( self . text_test ) } examples loaded in test set\" , color = typer . colors . BLUE , ) Model \u00b6 Main model class (data + model architecture + training) Parameters: Name Type Description Default config config required filepath saving model directory required Usage: from techlandscape.model import Model from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model = Model ( cfg ) model . fit () model . save () ModelBuilder \u00b6 Build model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelBuilder from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_builder = ModelBuilder ( cfg ) model_builder . build () # check model model_builder . model . summary () Resources Google ML Guide on text-classification Keras text-classification Understanding CNN build ( self , embedding_matrix = None ) \u00b6 Instantiate model based on config file Source code in techlandscape/model.py def build ( self , embedding_matrix : dict = None ): \"\"\" Instantiate model based on config file \"\"\" assert self . cfg [ \"model\" ][ \"architecture\" ] in SupportedModels . _member_names_ if not self . model : if self . cfg [ \"model\" ][ \"architecture\" ] == SupportedModels . mlp . value : self . _build_mlp () elif self . cfg [ \"model\" ][ \"architecture\" ] == SupportedModels . cnn . value : # TODO handle embedding matrix properly self . _build_cnn ( embedding_matrix ) else : raise UnknownModel ( UNKNOWN_MODEL_MSG ) typer . secho ( f \" { ok } Model built (see self.model.summary() for details)\" , color = typer . colors . GREEN , ) else : typer . secho ( \"Model already built\" , color = typer . colors . YELLOW ) ModelCompiler \u00b6 Compile model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelCompiler from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_compiler = ModelCompiler ( cfg ) model_compiler . compile () # check model, e.g. loss model_compiler . model . loss compile ( self , embedding_matrix = None ) \u00b6 Compile model. Use config file to instantiate training components. Source code in techlandscape/model.py def compile ( self , embedding_matrix : dict = None ): \"\"\"Compile model. Use config file to instantiate training components.\"\"\" self . build ( embedding_matrix ) self . optimizer = Adam ( lr = float ( self . cfg [ \"model\" ][ \"optimizer\" ][ \"learning_rate\" ])) self . model . compile ( optimizer = self . optimizer , loss = self . cfg [ \"model\" ][ \"optimizer\" ][ \"loss\" ], metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . Precision (), tf . keras . metrics . Recall (), ], ) typer . secho ( f \" { ok } Model compiled (see self.model.summary() for details)\" , color = typer . colors . GREEN , ) ModelFitter \u00b6 Fit model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelFitter from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_fitter = ModelFitter ( cfg ) model_fitter . fit () # check model, e.g. history model_fitter . model . history fit ( self ) \u00b6 Fit model Source code in techlandscape/model.py def fit ( self ): \"\"\"Fit model\"\"\" self . compile () if self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"active\" ]: self . callbacks += [ EarlyStopping ( monitor = self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"monitor\" ], patience = self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"patience\" ], restore_best_weights = True , ) ] if self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"active\" ]: self . filepath_best = ( get_project_root () / Path ( self . cfg [ \"out\" ]) / Path ( \"model-best\" ) ) self . callbacks += [ tf . keras . callbacks . ModelCheckpoint ( filepath = self . filepath_best , monitor = self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"monitor\" ], save_best_only = True , verbose = self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"verbose\" ], ) ] if self . cfg [ \"logger\" ][ \"tensorboard\" ][ \"active\" ]: self . logdir = get_project_root () / Path ( self . cfg [ \"logger\" ][ \"tensorboard\" ][ \"logdir\" ] ) self . callbacks += [ tf . keras . callbacks . TensorBoard ( self . logdir )] if not self . model . history : # if self.model_architecture == \"mlp\": # self.x_train = tf.sparse.reorder(self._convert_sparse_matrix_to_sparse_tensor(self.x_train)) # self.x_test = tf.sparse.reorder(self._convert_sparse_matrix_to_sparse_tensor(self.x_test)) self . model . fit ( self . x_train , self . y_train , epochs = self . cfg [ \"training\" ][ \"epochs\" ], callbacks = self . callbacks , validation_data = ( self . x_test , self . y_test ), verbose = self . cfg [ \"logger\" ][ \"verbose\" ], batch_size = self . cfg [ \"training\" ][ \"batch_size\" ], ) typer . secho ( f \" { ok } Model trained\" , color = typer . colors . GREEN ) else : # Alternative: clear session (keras.backend.clear_session()) and retrain typer . secho ( f \"Model already trained\" , color = typer . colors . YELLOW ) TextVectorizer \u00b6 Vectorize data Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import TextVectorizer from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) text_loader = TextVectorizer ( cfg ) text_loader . vectorize () # check examples text_loader . x_train vectorize ( self ) \u00b6 Return vectorized texts (train and test) Source code in techlandscape/model.py def vectorize ( self ): \"\"\"Return vectorized texts (train and test)\"\"\" if self . model_architecture == \"cnn\" : self . _get_sequences () elif self . model_architecture == \"mlp\" : self . _get_ngrams () else : raise UnknownModel ( UNKNOWN_MODEL_MSG ) typer . secho ( f \" { ok } Text vectorized\" , color = typer . colors . GREEN ) train ( cfg ) \u00b6 Train and save mode Source code in techlandscape/model.py @hydra . main ( config_path = \"../configs\" ) def train ( cfg : DictConfig ) -> None : \"\"\" Train and save mode \"\"\" model = Model ( config = cfg ) model . fit () model . save_meta () model . save_config ()","title":"model"},{"location":"API_MODEL/#techlandscape.model.DataLoader","text":"Load data (train, test) Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import DataLoader from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) data_loader = DataLoader ( cfg ) data_loader . load () # check examples data_loader . text_train","title":"DataLoader"},{"location":"API_MODEL/#techlandscape.model.DataLoader.load","text":"Load data. Expect a jsonl file where each row at least two fields: 'text' and 'is_seed'. Source code in techlandscape/model.py def load ( self ): \"\"\"Load data. Expect a jsonl file where each row at least two fields: 'text' and 'is_seed'.\"\"\" if any ( map ( lambda x : x is None , [ self . text_train , self . text_test , self . y_train , self . y_test ], ) ): self . text_train = self . _get_data ( self . train_path , \"text\" ) self . text_test = self . _get_data ( self . test_path , \"text\" ) self . y_train = np . array ( self . _get_data ( self . train_path , \"is_seed\" )) . astype ( int ) try : self . y_test = np . array ( self . _get_data ( self . test_path , \"is_seed\" ) ) . astype ( int ) except KeyError : typer . secho ( \"No output variable in test. You can still vectorize the data.\" , color = typer . colors . YELLOW , ) pass typer . secho ( f \" { ok } Data loaded\" , color = typer . colors . GREEN ) else : typer . secho ( \"Data already populated\" , color = typer . colors . YELLOW ) typer . secho ( f \" { ok }{ len ( self . text_train ) } examples loaded in training set\" , color = typer . colors . BLUE , ) typer . secho ( f \" { ok }{ len ( self . text_test ) } examples loaded in test set\" , color = typer . colors . BLUE , )","title":"load()"},{"location":"API_MODEL/#techlandscape.model.Model","text":"Main model class (data + model architecture + training) Parameters: Name Type Description Default config config required filepath saving model directory required Usage: from techlandscape.model import Model from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model = Model ( cfg ) model . fit () model . save ()","title":"Model"},{"location":"API_MODEL/#techlandscape.model.ModelBuilder","text":"Build model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelBuilder from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_builder = ModelBuilder ( cfg ) model_builder . build () # check model model_builder . model . summary () Resources Google ML Guide on text-classification Keras text-classification Understanding CNN","title":"ModelBuilder"},{"location":"API_MODEL/#techlandscape.model.ModelBuilder.build","text":"Instantiate model based on config file Source code in techlandscape/model.py def build ( self , embedding_matrix : dict = None ): \"\"\" Instantiate model based on config file \"\"\" assert self . cfg [ \"model\" ][ \"architecture\" ] in SupportedModels . _member_names_ if not self . model : if self . cfg [ \"model\" ][ \"architecture\" ] == SupportedModels . mlp . value : self . _build_mlp () elif self . cfg [ \"model\" ][ \"architecture\" ] == SupportedModels . cnn . value : # TODO handle embedding matrix properly self . _build_cnn ( embedding_matrix ) else : raise UnknownModel ( UNKNOWN_MODEL_MSG ) typer . secho ( f \" { ok } Model built (see self.model.summary() for details)\" , color = typer . colors . GREEN , ) else : typer . secho ( \"Model already built\" , color = typer . colors . YELLOW )","title":"build()"},{"location":"API_MODEL/#techlandscape.model.ModelCompiler","text":"Compile model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelCompiler from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_compiler = ModelCompiler ( cfg ) model_compiler . compile () # check model, e.g. loss model_compiler . model . loss","title":"ModelCompiler"},{"location":"API_MODEL/#techlandscape.model.ModelCompiler.compile","text":"Compile model. Use config file to instantiate training components. Source code in techlandscape/model.py def compile ( self , embedding_matrix : dict = None ): \"\"\"Compile model. Use config file to instantiate training components.\"\"\" self . build ( embedding_matrix ) self . optimizer = Adam ( lr = float ( self . cfg [ \"model\" ][ \"optimizer\" ][ \"learning_rate\" ])) self . model . compile ( optimizer = self . optimizer , loss = self . cfg [ \"model\" ][ \"optimizer\" ][ \"loss\" ], metrics = [ tf . keras . metrics . BinaryAccuracy (), tf . keras . metrics . Precision (), tf . keras . metrics . Recall (), ], ) typer . secho ( f \" { ok } Model compiled (see self.model.summary() for details)\" , color = typer . colors . GREEN , )","title":"compile()"},{"location":"API_MODEL/#techlandscape.model.ModelFitter","text":"Fit model Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import ModelFitter from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) model_fitter = ModelFitter ( cfg ) model_fitter . fit () # check model, e.g. history model_fitter . model . history","title":"ModelFitter"},{"location":"API_MODEL/#techlandscape.model.ModelFitter.fit","text":"Fit model Source code in techlandscape/model.py def fit ( self ): \"\"\"Fit model\"\"\" self . compile () if self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"active\" ]: self . callbacks += [ EarlyStopping ( monitor = self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"monitor\" ], patience = self . cfg [ \"training\" ][ \"callbacks\" ][ \"early_stopping\" ][ \"patience\" ], restore_best_weights = True , ) ] if self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"active\" ]: self . filepath_best = ( get_project_root () / Path ( self . cfg [ \"out\" ]) / Path ( \"model-best\" ) ) self . callbacks += [ tf . keras . callbacks . ModelCheckpoint ( filepath = self . filepath_best , monitor = self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"monitor\" ], save_best_only = True , verbose = self . cfg [ \"training\" ][ \"callbacks\" ][ \"save_best_only\" ][ \"verbose\" ], ) ] if self . cfg [ \"logger\" ][ \"tensorboard\" ][ \"active\" ]: self . logdir = get_project_root () / Path ( self . cfg [ \"logger\" ][ \"tensorboard\" ][ \"logdir\" ] ) self . callbacks += [ tf . keras . callbacks . TensorBoard ( self . logdir )] if not self . model . history : # if self.model_architecture == \"mlp\": # self.x_train = tf.sparse.reorder(self._convert_sparse_matrix_to_sparse_tensor(self.x_train)) # self.x_test = tf.sparse.reorder(self._convert_sparse_matrix_to_sparse_tensor(self.x_test)) self . model . fit ( self . x_train , self . y_train , epochs = self . cfg [ \"training\" ][ \"epochs\" ], callbacks = self . callbacks , validation_data = ( self . x_test , self . y_test ), verbose = self . cfg [ \"logger\" ][ \"verbose\" ], batch_size = self . cfg [ \"training\" ][ \"batch_size\" ], ) typer . secho ( f \" { ok } Model trained\" , color = typer . colors . GREEN ) else : # Alternative: clear session (keras.backend.clear_session()) and retrain typer . secho ( f \"Model already trained\" , color = typer . colors . YELLOW )","title":"fit()"},{"location":"API_MODEL/#techlandscape.model.TextVectorizer","text":"Vectorize data Parameters: Name Type Description Default config config file path required Usage: from techlandscape.model import TextVectorizer from techlandscape.utils import get_config cfg = get_config ( \"configs/model_cnn/default.yaml\" ) cfg . update ({ \"data\" : { \"train\" : \"your-train.jsonl\" , \"test\" : \"your-test.jsonl\" }, \"out\" : \"your-save-dir\" }) text_loader = TextVectorizer ( cfg ) text_loader . vectorize () # check examples text_loader . x_train","title":"TextVectorizer"},{"location":"API_MODEL/#techlandscape.model.TextVectorizer.vectorize","text":"Return vectorized texts (train and test) Source code in techlandscape/model.py def vectorize ( self ): \"\"\"Return vectorized texts (train and test)\"\"\" if self . model_architecture == \"cnn\" : self . _get_sequences () elif self . model_architecture == \"mlp\" : self . _get_ngrams () else : raise UnknownModel ( UNKNOWN_MODEL_MSG ) typer . secho ( f \" { ok } Text vectorized\" , color = typer . colors . GREEN )","title":"vectorize()"},{"location":"API_MODEL/#techlandscape.model.train","text":"Train and save mode Source code in techlandscape/model.py @hydra . main ( config_path = \"../configs\" ) def train ( cfg : DictConfig ) -> None : \"\"\" Train and save mode \"\"\" model = Model ( config = cfg ) model . fit () model . save_meta () model . save_config ()","title":"train()"},{"location":"API_MODELARCHITECTURES/","text":"MODEL ARCHITECTURES \u00b6 A Model is made of data, a tok2vec, a model architecture and training specificities. This page documents the built-in models used for the pruning step and how they can be used/parametrized through human-readable config files. Info Under the hood, we use keras. Components model and training can easily be extended to other keras parameters. data \u00b6 The data component refers to the data used for training/testing the model. Name Description Type train Training data file path Path test Test data file path Path Example data : train : data/train.jsonl test : data/test.jsonl Data format train/test are expected to be JSONL files where each row contains a field \"text\" (the abstract) and a field \"is_seed\" (the manual annotation) with values 1 if the candidate was classified in the seed, 0 otherwise. # Example { \"text\" : \"fluctuat nec mergitur\" , \"is_seed\" : 1 } tok2vec \u00b6 The tok2vec component refers to the text vectorization stage - that is the process going from raw text to data that can be ingested by the model. mlp In the mlp case, we vectorize the text data using the top_k features of a TfIdfVectorizer fitted on the training data. The top_k features are determined by the f_classif . Name Description Recommended values Type ngram_range The lower and upper boundary of the range of n-values for different n-grams to be extracted. [1,2] , [1,3] List[int] dtype Type of the matrix returned. float32 numeric strip_accents Remove accents and perform other character normalization during the preprocessing step. \"unicode\" is slightly slower but works on any character. \"unicode\" {\"ascii\", \"unicode\"} decode_error Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. \"replace\" {\"strict\", \"ignore\", \"replace\"} analyzer Whether the feature should be made of word or character n-grams. \"word\" {\"word\", \"char\", \"char_wb\"} min_df When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. 1 , 2 , 4 int top_k Select features according to the k f_classif highest scores. 5000 int Example tok2vec : ngram_range : [ 1 , 2 ] dtype : float32 strip_accents : unicode decode_error : replace analyzer : word min_df : 2 top_k : 5000 cnn In the cnn case, we tokenize the text data using keras.preprocessing.text.Tokenizer and truncate/padd the sequence to max_length . only the top_k tokens (by frequency) are used. Name Description Recommended values Type max_length Max length of a tokenized text seqence. Above that figure, sequence is truncated, below, it is padded. 250 int top_k Only the most common num_words-1 words will be kept. 5000 int Example max_length : 250 top_k : 5000 model \u00b6 The model component defines the model's architecture, hyper-parameters and optimizer. mlp Name Description Recommended values Type architecture High level model architecture family name. \"mlp\" \"mlp\" layers Number of hidden layers. Nb: if 1, then logistic regression. 1 , 2 , 4 int units Number of units per hidden layer. 16 , 32 , 64 int dropout_rate Fraction of the input units to drop. 0 , 0.2 float Note Dropout is applied to hidden layers only. Example model : architecture : mlp layers : 1 units : 64 dropout_rate : .2 cnn Name Description Recommended values Type architecture High level model architecture family name. \"cnn\" \"cnn\" blocks Number of Convolution-Pooling pairs. 1 , 2 , 4 int filters Dimensionality of the output space (i.e. the number of output filters in the convolution). 16 , 32 , 64 int dropout_rate Percentage of input units to drop at Dropout Layer. 0 , 0.2 float embedding_dim Dimension of the embedding vectors (reco between 50 and 200). 50 , 100 , 200 int kernel_size Length of the 1D convolution window. 2 , 4 , 8 int pool_size Factor by which to downscale input at MaxPooling layer. 2 , 3 , 4 int use_pretrained_embedding True if pre-trained. Else False. False bool is_embedding_trainable Used only use_pretrained_embedding . True if pre-trained embedding is trainable. Else False False bool Example model : architecture : cnn blocks : 2 filters : 64 dropout_rate : .2 embedding_dim : 100 kernel_size : 5 pool_size : 3 use_pretrained_embedding : False is_embedding_trainable : False Optimizer \u00b6 The optimizer is nested in the model section. The same parameters are available for both architectures Name Description Recommended values Type learning_rate Learning rate. 1e-3 float loss Loss of the output layer. \"binary_crossentropy\" is warmly recommended since we are in a binary classification setting. See keras doc for available losses \"binary_crossentropy\" str metrics Metrics to record in model.history (also used in callbacks for early stopping). See keras doc for available metrics. [\"accuracy\"] List[str] Example model : ... optimizer : learning_rate : 1e-3 loss : binary_crossentropy metrics : [ \"accuracy\" ] Training \u00b6 The training component sets the training parameters. Name Description Recommended values Type epochs Number of training epochs (full training dataset iteration). 20 , 50 , 100 int batch_size Size of the batch used for between 2 model updates. 16 , 32 , 64 int Example training : epochs : 100 batch_size : 64 Callbacks \u00b6 Callbacks component is nested in training. It sets the parameters used for early-stopping. Seed keras doc for more. Name Description Recommended values Type monitor Quantity to be monitored. \"val_loss\" \"val_loss\" patience Number of epochs with no improvement after which training will be stopped. 2 , 5 int Example training : ... callbacks : monitor : val_loss patience : 2 Logger \u00b6 The component logger sets the verbosity level of the model training. Name Description Recommended values Type verbose Verbosity level of the model training. 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 2 int Example logger : verbose : 2","title":"model architectures"},{"location":"API_MODELARCHITECTURES/#model-architectures","text":"A Model is made of data, a tok2vec, a model architecture and training specificities. This page documents the built-in models used for the pruning step and how they can be used/parametrized through human-readable config files. Info Under the hood, we use keras. Components model and training can easily be extended to other keras parameters.","title":"MODEL ARCHITECTURES"},{"location":"API_MODELARCHITECTURES/#data","text":"The data component refers to the data used for training/testing the model. Name Description Type train Training data file path Path test Test data file path Path Example data : train : data/train.jsonl test : data/test.jsonl Data format train/test are expected to be JSONL files where each row contains a field \"text\" (the abstract) and a field \"is_seed\" (the manual annotation) with values 1 if the candidate was classified in the seed, 0 otherwise. # Example { \"text\" : \"fluctuat nec mergitur\" , \"is_seed\" : 1 }","title":"data"},{"location":"API_MODELARCHITECTURES/#tok2vec","text":"The tok2vec component refers to the text vectorization stage - that is the process going from raw text to data that can be ingested by the model. mlp In the mlp case, we vectorize the text data using the top_k features of a TfIdfVectorizer fitted on the training data. The top_k features are determined by the f_classif . Name Description Recommended values Type ngram_range The lower and upper boundary of the range of n-values for different n-grams to be extracted. [1,2] , [1,3] List[int] dtype Type of the matrix returned. float32 numeric strip_accents Remove accents and perform other character normalization during the preprocessing step. \"unicode\" is slightly slower but works on any character. \"unicode\" {\"ascii\", \"unicode\"} decode_error Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. \"replace\" {\"strict\", \"ignore\", \"replace\"} analyzer Whether the feature should be made of word or character n-grams. \"word\" {\"word\", \"char\", \"char_wb\"} min_df When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. 1 , 2 , 4 int top_k Select features according to the k f_classif highest scores. 5000 int Example tok2vec : ngram_range : [ 1 , 2 ] dtype : float32 strip_accents : unicode decode_error : replace analyzer : word min_df : 2 top_k : 5000 cnn In the cnn case, we tokenize the text data using keras.preprocessing.text.Tokenizer and truncate/padd the sequence to max_length . only the top_k tokens (by frequency) are used. Name Description Recommended values Type max_length Max length of a tokenized text seqence. Above that figure, sequence is truncated, below, it is padded. 250 int top_k Only the most common num_words-1 words will be kept. 5000 int Example max_length : 250 top_k : 5000","title":"tok2vec"},{"location":"API_MODELARCHITECTURES/#model","text":"The model component defines the model's architecture, hyper-parameters and optimizer. mlp Name Description Recommended values Type architecture High level model architecture family name. \"mlp\" \"mlp\" layers Number of hidden layers. Nb: if 1, then logistic regression. 1 , 2 , 4 int units Number of units per hidden layer. 16 , 32 , 64 int dropout_rate Fraction of the input units to drop. 0 , 0.2 float Note Dropout is applied to hidden layers only. Example model : architecture : mlp layers : 1 units : 64 dropout_rate : .2 cnn Name Description Recommended values Type architecture High level model architecture family name. \"cnn\" \"cnn\" blocks Number of Convolution-Pooling pairs. 1 , 2 , 4 int filters Dimensionality of the output space (i.e. the number of output filters in the convolution). 16 , 32 , 64 int dropout_rate Percentage of input units to drop at Dropout Layer. 0 , 0.2 float embedding_dim Dimension of the embedding vectors (reco between 50 and 200). 50 , 100 , 200 int kernel_size Length of the 1D convolution window. 2 , 4 , 8 int pool_size Factor by which to downscale input at MaxPooling layer. 2 , 3 , 4 int use_pretrained_embedding True if pre-trained. Else False. False bool is_embedding_trainable Used only use_pretrained_embedding . True if pre-trained embedding is trainable. Else False False bool Example model : architecture : cnn blocks : 2 filters : 64 dropout_rate : .2 embedding_dim : 100 kernel_size : 5 pool_size : 3 use_pretrained_embedding : False is_embedding_trainable : False","title":"model"},{"location":"API_MODELARCHITECTURES/#optimizer","text":"The optimizer is nested in the model section. The same parameters are available for both architectures Name Description Recommended values Type learning_rate Learning rate. 1e-3 float loss Loss of the output layer. \"binary_crossentropy\" is warmly recommended since we are in a binary classification setting. See keras doc for available losses \"binary_crossentropy\" str metrics Metrics to record in model.history (also used in callbacks for early stopping). See keras doc for available metrics. [\"accuracy\"] List[str] Example model : ... optimizer : learning_rate : 1e-3 loss : binary_crossentropy metrics : [ \"accuracy\" ]","title":"Optimizer"},{"location":"API_MODELARCHITECTURES/#training","text":"The training component sets the training parameters. Name Description Recommended values Type epochs Number of training epochs (full training dataset iteration). 20 , 50 , 100 int batch_size Size of the batch used for between 2 model updates. 16 , 32 , 64 int Example training : epochs : 100 batch_size : 64","title":"Training"},{"location":"API_MODELARCHITECTURES/#callbacks","text":"Callbacks component is nested in training. It sets the parameters used for early-stopping. Seed keras doc for more. Name Description Recommended values Type monitor Quantity to be monitored. \"val_loss\" \"val_loss\" patience Number of epochs with no improvement after which training will be stopped. 2 , 5 int Example training : ... callbacks : monitor : val_loss patience : 2","title":"Callbacks"},{"location":"API_MODELARCHITECTURES/#logger","text":"The component logger sets the verbosity level of the model training. Name Description Recommended values Type verbose Verbosity level of the model training. 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 2 int Example logger : verbose : 2","title":"Logger"},{"location":"API_ROBUSTNESS/","text":"get_overlap_analysis ( technology , kind , credentials , summary = False , destination = None , robustness_dataset = 'robustness' ) \u00b6 Return the overlap analysis of technology Parameters: Name Type Description Default technology str name of the technology (as required kind OverlapAnalysisKind kind of overlap analysis required credentials Path BQ credentials file path required destination Path results destination file path (if None, stdout) None summary bool whether the full analysis or its summary should be saved False robustness_dataset str name of the BQ 'robustness' dataset 'robustness' Usage: techlandscape robustness get-overlap-analysis <technology> <your-credentials> --destination <overlap_analysis.csv> Source code in techlandscape/robustness.py @app . command () def get_overlap_analysis ( technology : str , kind : OverlapAnalysisKind , credentials : Path , summary : bool = False , destination : Path = None , robustness_dataset : str = \"robustness\" , ): \"\"\" Return the overlap analysis of `technology` Arguments: technology: name of the technology (as kind: kind of overlap analysis credentials: BQ credentials file path destination: results destination file path (if None, stdout) summary: whether the full analysis or its summary should be saved robustness_dataset: name of the BQ 'robustness' dataset **Usage:** ```shell techlandscape robustness get-overlap-analysis <technology> <your-credentials> --destination <overlap_analysis.csv> ``` \"\"\" overlap_analysis = OverlapAnalysis ( technology , credentials , robustness_dataset ) if kind == OverlapAnalysisKind . pairwise : if summary : overlap_analysis . get_pairwise_overlap_ratios () res , index , header = ( overlap_analysis . pairwise_overlap_ratios . describe (), True , False , ) else : overlap_analysis . get_pairwise_overlap_analysis () res , index , header = overlap_analysis . pairwise_overlap_analysis , False , True else : if summary : overlap_analysis . get_batch_overlap_analysis () res , index , header = ( overlap_analysis . batch_overlap_ratios . describe (), True , False , ) else : overlap_analysis . get_batch_overlap_analysis () res , index , header = overlap_analysis . batch_overlap_ratios , False , True destination = destination if destination else sys . stdout res . to_csv ( destination , index = index , header = header ) get_prediction_analysis ( models , data , destination = None ) \u00b6 Return a csv file with predicted scores on data for all models matching the models pattern. Parameters: Name Type Description Default models str model folder path (wildcard enabled) required data str data file path required destination Path destination file path None Usage: techlandscape robustness get-prediction-analysis \"models/additivemanufacturing_*_cnn/model-best\" data/expansion_additivemanufacturing_sample.jsonl --destination outs/ # will be saved as classification_additivemanufacturing_robustness_cnn.csv Source code in techlandscape/robustness.py @app . command () def get_prediction_analysis ( models : str , data : str , destination : Path = None ): \"\"\" Return a csv file with predicted scores on `data` for all models matching the `models` pattern. Arguments: models: model folder path (wildcard enabled) data: data file path destination: destination file path **Usage:** ```shell techlandscape robustness get-prediction-analysis \"models/additivemanufacturing_*_cnn/model-best\" data/expansion_additivemanufacturing_sample.jsonl --destination outs/ # will be saved as classification_additivemanufacturing_robustness_cnn.csv ``` \"\"\" get_technology = lambda x : x . split ( \"/\" )[ - 2 ] . split ( \"_\" )[ 0 ] get_architecture = lambda x : x . split ( \"/\" )[ - 2 ] . split ( \"_\" )[ - 1 ] models = glob ( models ) for i , model_ in enumerate ( models ): technology = get_technology ( model_ ) architecture = get_architecture ( model_ ) model = tf . keras . models . load_model ( model_ ) cfg = get_config ( Path ( model_ ) / Path ( \"config.yaml\" )) cfg [ \"data\" ][ \"test\" ] = data text_vectorizer = TextVectorizer ( cfg ) text_vectorizer . vectorize () pred = model . predict ( text_vectorizer . x_test ) if i == 0 : out = pd . DataFrame ( pred , columns = [ model_ ]) else : out = out . merge ( pd . DataFrame ( pred , columns = [ model_ ]), left_index = True , right_index = True ) filename = f \"classification_ { technology } _robustness_ { architecture } .csv\" out . to_csv ( Path ( destination ) / Path ( filename )) typer . secho ( f \" { ok }{ Path ( destination ) / Path ( filename ) } saved\" ) models_performance ( path , markdown = True , destination = None , title = None ) \u00b6 Summarize models performance and save to csv/ print to stdout Parameters: Name Type Description Default path str path of the meta.json (wildcard enabled) required markdown bool whether the output should be printed to stdout as md or saved to destination True destination str destination file path (used if --no-markdown ) None title str title of the table (used if --markdown ) None Usage: techlandscape robustness models-performance \"models/additivemanufacturing_*_cnn/model-best/meta.json\" --markdown --title \"additivemanufacturing - cnn\" Source code in techlandscape/robustness.py @app . command () def models_performance ( path : str , markdown : bool = True , destination : str = None , title : str = None ): \"\"\" Summarize models performance and save to csv/ print to stdout Arguments: path: path of the meta.json (wildcard enabled) markdown: whether the output should be printed to stdout as md or saved to `destination` destination: destination file path (used if `--no-markdown`) title: title of the table (used if `--markdown`) **Usage:** ```shell techlandscape robustness models-performance \"models/additivemanufacturing_*_cnn/model-best/meta.json\" --markdown --title \"additivemanufacturing - cnn\" ``` \"\"\" files = glob ( path ) get_name = lambda x : x . split ( \"/\" )[ 1 ] for i , file in enumerate ( files ): tmp = pd . DataFrame . from_dict ( json . loads ( Path ( file ) . open ( \"r\" ) . read ())) . rename ( columns = { \"performance\" : get_name ( file )} ) if i == 0 : out = tmp . copy () else : out = out . merge ( tmp , left_index = True , right_index = True ) out = out . T out = out [ sorted ( out . columns )] if len ( files ) > 1 : out = out . describe () if markdown : typer . echo ( f \" \\n ### { title } \\n \" ) typer . echo ( f \" { out . round ( 2 ) . to_markdown () } \" ) else : out . to_csv ( destination ) wrap_overlap_analysis ( path , axis , destination = None , markdown = False ) \u00b6 Wrap overlap analysis based on csv output of get_overlap_analysis Parameters: Name Type Description Default path str path of the files with results to be wrapped (wildcard enablec) required axis OverlapAnalysisAxis axis of the main analysis required destination str saving file path (print to stdout in None) None markdown bool whether to return as md or csv table False Usage: techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" --markdown Source code in techlandscape/robustness.py @app . command () def wrap_overlap_analysis ( path : str , axis : OverlapAnalysisAxis , destination : str = None , markdown : bool = False , ): \"\"\" Wrap overlap analysis based on csv output of `get_overlap_analysis` Arguments: path: path of the files with results to be wrapped (wildcard enablec) axis: axis of the main analysis destination: saving file path (print to stdout in None) markdown: whether to return as md or csv table **Usage:** ```shell techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" --markdown ``` \"\"\" files = glob ( path ) get_technology = lambda f : f . split ( \"_\" )[ 1 ] get_config = lambda f : f . split ( \"_\" )[ 2 ] . replace ( \".csv\" , \"\" ) technologies = sorted ( set ([ get_technology ( f ) for f in files ])) configs = sorted ( set ([ get_config ( f ) for f in files ])) for e in eval ( axis . value ): files_ = [ f for f in files if e in f ] tmp = pd . DataFrame () for file in files_ : name = ( get_config ( file ) if axis == OverlapAnalysisAxis . technologies else get_technology ( file ) ) tmp = tmp . append ( pd . read_csv ( file , names = [ \"var\" , name ]) . set_index ( \"var\" ) . T ) tmp . index . name = ( \"technologies\" if axis == OverlapAnalysisAxis . configs else \"configs\" ) tmp = tmp . sort_index () . round ( 2 ) out = destination if destination else sys . stdout if markdown : typer . echo ( f \" \\n\\n ## { e } \\n \" ) tmp . to_markdown ( out ) else : tmp . to_csv ( out ) wrap_prediction_analysis ( path , markdown = True ) \u00b6 Wrap prediction analysis Parameters: Name Type Description Default path str prediction analysis file path (wildcard enabled) required markdown bool whether to output wrapped analysis as markdown or csv True Attention csv not supported yet Usage: techlandscape robustness wrap-prediction-analysis outs/classification_additivemanufacturing_robustness_cnn.csv Source code in techlandscape/robustness.py @app . command () def wrap_prediction_analysis ( path : str , markdown : bool = True ): \"\"\" Wrap prediction analysis Arguments: path: prediction analysis file path (wildcard enabled) markdown: whether to output wrapped analysis as markdown or csv !!! attention csv not supported yet **Usage:** ```shell techlandscape robustness wrap-prediction-analysis outs/classification_additivemanufacturing_robustness_cnn.csv ``` \"\"\" get_technology = lambda x : x . split ( \"/\" )[ 1 ] . split ( \"_\" )[ 1 ] get_architecture = lambda x : x . split ( \"/\" )[ 1 ] . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] files = glob ( path ) files = sorted ( files ) for file in files : technology = get_technology ( file ) architecture = get_architecture ( file ) tmp = pd . read_csv ( file , index_col = 0 ) dispersion = tmp . std ( axis = 1 ) . describe () . rename ( \"std_score\" ) . copy () for col in tmp . columns : tmp [ col ] = tmp [ col ] . apply ( lambda x : 1 if x > 0.5 else 0 ) tmp [ \"vote\" ] = tmp . sum ( 1 ) tmp = ( tmp . groupby ( \"vote\" ) . count () . max ( 1 ) . to_frame () . reset_index () . prod ( 1 ) . rename ( \"nb_positives\" ) . to_frame () ) tmp [ \"share_positives\" ] = tmp [ \"nb_positives\" ] / tmp [ \"nb_positives\" ] . sum () tmp = tmp [:: - 1 ] tmp [ \"cumshare_positives\" ] = tmp [ \"share_positives\" ] . cumsum () tmp . index . name = \"nb_models\" consensus = tmp . copy () if markdown : typer . echo ( f \" \\n ## { technology } - { architecture } \\n \" ) typer . echo ( \"### Score dispersion \\n \" ) typer . echo ( dispersion . round ( 3 ) . to_markdown () + \" \\n \" ) typer . echo ( \"### Models consensus \\n \" ) typer . echo ( consensus . round ( 3 ) . to_markdown ()) else : typer . secho ( f \" { not_ok } csv not supported yet\" , err = True )","title":"robustness"},{"location":"API_ROBUSTNESS/#techlandscape.robustness.get_overlap_analysis","text":"Return the overlap analysis of technology Parameters: Name Type Description Default technology str name of the technology (as required kind OverlapAnalysisKind kind of overlap analysis required credentials Path BQ credentials file path required destination Path results destination file path (if None, stdout) None summary bool whether the full analysis or its summary should be saved False robustness_dataset str name of the BQ 'robustness' dataset 'robustness' Usage: techlandscape robustness get-overlap-analysis <technology> <your-credentials> --destination <overlap_analysis.csv> Source code in techlandscape/robustness.py @app . command () def get_overlap_analysis ( technology : str , kind : OverlapAnalysisKind , credentials : Path , summary : bool = False , destination : Path = None , robustness_dataset : str = \"robustness\" , ): \"\"\" Return the overlap analysis of `technology` Arguments: technology: name of the technology (as kind: kind of overlap analysis credentials: BQ credentials file path destination: results destination file path (if None, stdout) summary: whether the full analysis or its summary should be saved robustness_dataset: name of the BQ 'robustness' dataset **Usage:** ```shell techlandscape robustness get-overlap-analysis <technology> <your-credentials> --destination <overlap_analysis.csv> ``` \"\"\" overlap_analysis = OverlapAnalysis ( technology , credentials , robustness_dataset ) if kind == OverlapAnalysisKind . pairwise : if summary : overlap_analysis . get_pairwise_overlap_ratios () res , index , header = ( overlap_analysis . pairwise_overlap_ratios . describe (), True , False , ) else : overlap_analysis . get_pairwise_overlap_analysis () res , index , header = overlap_analysis . pairwise_overlap_analysis , False , True else : if summary : overlap_analysis . get_batch_overlap_analysis () res , index , header = ( overlap_analysis . batch_overlap_ratios . describe (), True , False , ) else : overlap_analysis . get_batch_overlap_analysis () res , index , header = overlap_analysis . batch_overlap_ratios , False , True destination = destination if destination else sys . stdout res . to_csv ( destination , index = index , header = header )","title":"get_overlap_analysis()"},{"location":"API_ROBUSTNESS/#techlandscape.robustness.get_prediction_analysis","text":"Return a csv file with predicted scores on data for all models matching the models pattern. Parameters: Name Type Description Default models str model folder path (wildcard enabled) required data str data file path required destination Path destination file path None Usage: techlandscape robustness get-prediction-analysis \"models/additivemanufacturing_*_cnn/model-best\" data/expansion_additivemanufacturing_sample.jsonl --destination outs/ # will be saved as classification_additivemanufacturing_robustness_cnn.csv Source code in techlandscape/robustness.py @app . command () def get_prediction_analysis ( models : str , data : str , destination : Path = None ): \"\"\" Return a csv file with predicted scores on `data` for all models matching the `models` pattern. Arguments: models: model folder path (wildcard enabled) data: data file path destination: destination file path **Usage:** ```shell techlandscape robustness get-prediction-analysis \"models/additivemanufacturing_*_cnn/model-best\" data/expansion_additivemanufacturing_sample.jsonl --destination outs/ # will be saved as classification_additivemanufacturing_robustness_cnn.csv ``` \"\"\" get_technology = lambda x : x . split ( \"/\" )[ - 2 ] . split ( \"_\" )[ 0 ] get_architecture = lambda x : x . split ( \"/\" )[ - 2 ] . split ( \"_\" )[ - 1 ] models = glob ( models ) for i , model_ in enumerate ( models ): technology = get_technology ( model_ ) architecture = get_architecture ( model_ ) model = tf . keras . models . load_model ( model_ ) cfg = get_config ( Path ( model_ ) / Path ( \"config.yaml\" )) cfg [ \"data\" ][ \"test\" ] = data text_vectorizer = TextVectorizer ( cfg ) text_vectorizer . vectorize () pred = model . predict ( text_vectorizer . x_test ) if i == 0 : out = pd . DataFrame ( pred , columns = [ model_ ]) else : out = out . merge ( pd . DataFrame ( pred , columns = [ model_ ]), left_index = True , right_index = True ) filename = f \"classification_ { technology } _robustness_ { architecture } .csv\" out . to_csv ( Path ( destination ) / Path ( filename )) typer . secho ( f \" { ok }{ Path ( destination ) / Path ( filename ) } saved\" )","title":"get_prediction_analysis()"},{"location":"API_ROBUSTNESS/#techlandscape.robustness.models_performance","text":"Summarize models performance and save to csv/ print to stdout Parameters: Name Type Description Default path str path of the meta.json (wildcard enabled) required markdown bool whether the output should be printed to stdout as md or saved to destination True destination str destination file path (used if --no-markdown ) None title str title of the table (used if --markdown ) None Usage: techlandscape robustness models-performance \"models/additivemanufacturing_*_cnn/model-best/meta.json\" --markdown --title \"additivemanufacturing - cnn\" Source code in techlandscape/robustness.py @app . command () def models_performance ( path : str , markdown : bool = True , destination : str = None , title : str = None ): \"\"\" Summarize models performance and save to csv/ print to stdout Arguments: path: path of the meta.json (wildcard enabled) markdown: whether the output should be printed to stdout as md or saved to `destination` destination: destination file path (used if `--no-markdown`) title: title of the table (used if `--markdown`) **Usage:** ```shell techlandscape robustness models-performance \"models/additivemanufacturing_*_cnn/model-best/meta.json\" --markdown --title \"additivemanufacturing - cnn\" ``` \"\"\" files = glob ( path ) get_name = lambda x : x . split ( \"/\" )[ 1 ] for i , file in enumerate ( files ): tmp = pd . DataFrame . from_dict ( json . loads ( Path ( file ) . open ( \"r\" ) . read ())) . rename ( columns = { \"performance\" : get_name ( file )} ) if i == 0 : out = tmp . copy () else : out = out . merge ( tmp , left_index = True , right_index = True ) out = out . T out = out [ sorted ( out . columns )] if len ( files ) > 1 : out = out . describe () if markdown : typer . echo ( f \" \\n ### { title } \\n \" ) typer . echo ( f \" { out . round ( 2 ) . to_markdown () } \" ) else : out . to_csv ( destination )","title":"models_performance()"},{"location":"API_ROBUSTNESS/#techlandscape.robustness.wrap_overlap_analysis","text":"Wrap overlap analysis based on csv output of get_overlap_analysis Parameters: Name Type Description Default path str path of the files with results to be wrapped (wildcard enablec) required axis OverlapAnalysisAxis axis of the main analysis required destination str saving file path (print to stdout in None) None markdown bool whether to return as md or csv table False Usage: techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" --markdown Source code in techlandscape/robustness.py @app . command () def wrap_overlap_analysis ( path : str , axis : OverlapAnalysisAxis , destination : str = None , markdown : bool = False , ): \"\"\" Wrap overlap analysis based on csv output of `get_overlap_analysis` Arguments: path: path of the files with results to be wrapped (wildcard enablec) axis: axis of the main analysis destination: saving file path (print to stdout in None) markdown: whether to return as md or csv table **Usage:** ```shell techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" --markdown ``` \"\"\" files = glob ( path ) get_technology = lambda f : f . split ( \"_\" )[ 1 ] get_config = lambda f : f . split ( \"_\" )[ 2 ] . replace ( \".csv\" , \"\" ) technologies = sorted ( set ([ get_technology ( f ) for f in files ])) configs = sorted ( set ([ get_config ( f ) for f in files ])) for e in eval ( axis . value ): files_ = [ f for f in files if e in f ] tmp = pd . DataFrame () for file in files_ : name = ( get_config ( file ) if axis == OverlapAnalysisAxis . technologies else get_technology ( file ) ) tmp = tmp . append ( pd . read_csv ( file , names = [ \"var\" , name ]) . set_index ( \"var\" ) . T ) tmp . index . name = ( \"technologies\" if axis == OverlapAnalysisAxis . configs else \"configs\" ) tmp = tmp . sort_index () . round ( 2 ) out = destination if destination else sys . stdout if markdown : typer . echo ( f \" \\n\\n ## { e } \\n \" ) tmp . to_markdown ( out ) else : tmp . to_csv ( out )","title":"wrap_overlap_analysis()"},{"location":"API_ROBUSTNESS/#techlandscape.robustness.wrap_prediction_analysis","text":"Wrap prediction analysis Parameters: Name Type Description Default path str prediction analysis file path (wildcard enabled) required markdown bool whether to output wrapped analysis as markdown or csv True Attention csv not supported yet Usage: techlandscape robustness wrap-prediction-analysis outs/classification_additivemanufacturing_robustness_cnn.csv Source code in techlandscape/robustness.py @app . command () def wrap_prediction_analysis ( path : str , markdown : bool = True ): \"\"\" Wrap prediction analysis Arguments: path: prediction analysis file path (wildcard enabled) markdown: whether to output wrapped analysis as markdown or csv !!! attention csv not supported yet **Usage:** ```shell techlandscape robustness wrap-prediction-analysis outs/classification_additivemanufacturing_robustness_cnn.csv ``` \"\"\" get_technology = lambda x : x . split ( \"/\" )[ 1 ] . split ( \"_\" )[ 1 ] get_architecture = lambda x : x . split ( \"/\" )[ 1 ] . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] files = glob ( path ) files = sorted ( files ) for file in files : technology = get_technology ( file ) architecture = get_architecture ( file ) tmp = pd . read_csv ( file , index_col = 0 ) dispersion = tmp . std ( axis = 1 ) . describe () . rename ( \"std_score\" ) . copy () for col in tmp . columns : tmp [ col ] = tmp [ col ] . apply ( lambda x : 1 if x > 0.5 else 0 ) tmp [ \"vote\" ] = tmp . sum ( 1 ) tmp = ( tmp . groupby ( \"vote\" ) . count () . max ( 1 ) . to_frame () . reset_index () . prod ( 1 ) . rename ( \"nb_positives\" ) . to_frame () ) tmp [ \"share_positives\" ] = tmp [ \"nb_positives\" ] / tmp [ \"nb_positives\" ] . sum () tmp = tmp [:: - 1 ] tmp [ \"cumshare_positives\" ] = tmp [ \"share_positives\" ] . cumsum () tmp . index . name = \"nb_models\" consensus = tmp . copy () if markdown : typer . echo ( f \" \\n ## { technology } - { architecture } \\n \" ) typer . echo ( \"### Score dispersion \\n \" ) typer . echo ( dispersion . round ( 3 ) . to_markdown () + \" \\n \" ) typer . echo ( \"### Models consensus \\n \" ) typer . echo ( consensus . round ( 3 ) . to_markdown ()) else : typer . secho ( f \" { not_ok } csv not supported yet\" , err = True )","title":"wrap_prediction_analysis()"},{"location":"LICENSE_CODE/","text":"Copyright 2021 Cyril VERLUISE and Antonin BERGEAUD Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Code"},{"location":"LICENSE_DATA/","text":"Creative Commons Attribution 4.0 International \u00b6 Creative Commons Corporation (\u201cCreative Commons\u201d) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \u201cas-is\u201d basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible. Using Creative Commons Public Licenses \u00b6 Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors . Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor\u2019s permission is not necessary for any reason\u2013for example, because of any applicable exception or limitation to copyright\u2013then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public . Creative Commons Attribution 4.0 International Public License \u00b6 By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions. Section 1 \u2013 Definitions. \u00b6 a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning. Section 2 \u2013 Scope. \u00b6 a. License grant. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: A. reproduce and Share the Licensed Material, in whole or in part; and B. produce, reproduce, and Share Adapted Material. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. Term. The term of this Public License is specified in Section 6(a). Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material. Downstream recipients. A. Offer from the Licensor \u2013 Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. B. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. Patent and trademark rights are not licensed under this Public License. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties. Section 3 \u2013 License Conditions. \u00b6 Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. If You Share the Licensed Material (including in modified form), You must: A. retain the following if it is supplied by the Licensor with the Licensed Material: i . identification of the creator ( s ) of the Licensed Material and any others designated to receive attribution , in any reasonable manner requested by the Licensor ( including by pseudonym if designated ) ; ii . a copyright notice ; iii . a notice that refers to this Public License ; iv . a notice that refers to the disclaimer of warranties ; v . a URI or hyperlink to the Licensed Material to the extent reasonably practicable ; B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License. Section 4 \u2013 Sui Generis Database Rights. \u00b6 Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights. Section 5 \u2013 Disclaimer of Warranties and Limitation of Liability. \u00b6 a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You. b. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability. Section 6 \u2013 Term and Termination. \u00b6 a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License. Section 7 \u2013 Other Terms and Conditions. \u00b6 a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License. Section 8 \u2013 Interpretation. \u00b6 a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies , Creative Commons does not authorize the use of the trademark \u201cCreative Commons\u201d or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org","title":"Data"},{"location":"LICENSE_DATA/#creative-commons-attribution-40-international","text":"Creative Commons Corporation (\u201cCreative Commons\u201d) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \u201cas-is\u201d basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.","title":"Creative Commons Attribution 4.0 International"},{"location":"LICENSE_DATA/#using-creative-commons-public-licenses","text":"Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors . Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor\u2019s permission is not necessary for any reason\u2013for example, because of any applicable exception or limitation to copyright\u2013then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public .","title":"Using Creative Commons Public Licenses"},{"location":"LICENSE_DATA/#creative-commons-attribution-40-international-public-license","text":"By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.","title":"Creative Commons Attribution 4.0 International Public License"},{"location":"LICENSE_DATA/#section-1-definitions","text":"a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.","title":"Section 1 \u2013 Definitions."},{"location":"LICENSE_DATA/#section-2-scope","text":"a. License grant. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: A. reproduce and Share the Licensed Material, in whole or in part; and B. produce, reproduce, and Share Adapted Material. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. Term. The term of this Public License is specified in Section 6(a). Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material. Downstream recipients. A. Offer from the Licensor \u2013 Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. B. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. Patent and trademark rights are not licensed under this Public License. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.","title":"Section 2 \u2013 Scope."},{"location":"LICENSE_DATA/#section-3-license-conditions","text":"Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. If You Share the Licensed Material (including in modified form), You must: A. retain the following if it is supplied by the Licensor with the Licensed Material: i . identification of the creator ( s ) of the Licensed Material and any others designated to receive attribution , in any reasonable manner requested by the Licensor ( including by pseudonym if designated ) ; ii . a copyright notice ; iii . a notice that refers to this Public License ; iv . a notice that refers to the disclaimer of warranties ; v . a URI or hyperlink to the Licensed Material to the extent reasonably practicable ; B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License.","title":"Section 3 \u2013 License Conditions."},{"location":"LICENSE_DATA/#section-4-sui-generis-database-rights","text":"Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.","title":"Section 4 \u2013 Sui Generis Database Rights."},{"location":"LICENSE_DATA/#section-5-disclaimer-of-warranties-and-limitation-of-liability","text":"a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You. b. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.","title":"Section 5 \u2013 Disclaimer of Warranties and Limitation of Liability."},{"location":"LICENSE_DATA/#section-6-term-and-termination","text":"a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.","title":"Section 6 \u2013 Term and Termination."},{"location":"LICENSE_DATA/#section-7-other-terms-and-conditions","text":"a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.","title":"Section 7 \u2013 Other Terms and Conditions."},{"location":"LICENSE_DATA/#section-8-interpretation","text":"a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies , Creative Commons does not authorize the use of the trademark \u201cCreative Commons\u201d or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org","title":"Section 8 \u2013 Interpretation."},{"location":"MODELS_PERFORMANCE/","text":"MODELS PERFORMANCE \u00b6 additivemanufacturing - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 2.46 0.78 0.82 std 0.03 0.42 0.07 0.07 min 0.82 1.68 0.69 0.69 25% 0.86 2.22 0.72 0.79 50% 0.88 2.49 0.79 0.85 75% 0.89 2.76 0.84 0.87 max 0.89 3 0.88 0.89 additivemanufacturing - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.3 0.89 0.79 std 0.02 0.05 0.04 0.05 min 0.87 0.21 0.82 0.72 25% 0.89 0.26 0.87 0.75 50% 0.9 0.31 0.89 0.79 75% 0.91 0.34 0.92 0.81 max 0.93 0.37 0.95 0.88 blockchain - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 1.11 0.84 0.89 std 0.03 0.51 0.05 0.06 min 0.85 0.34 0.76 0.8 25% 0.89 0.79 0.82 0.85 50% 0.91 1.12 0.83 0.88 75% 0.92 1.39 0.87 0.94 max 0.94 2.11 0.9 0.95 blockchain - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.91 0.22 0.91 0.82 std 0.01 0.04 0.03 0.04 min 0.89 0.18 0.87 0.77 25% 0.9 0.19 0.89 0.78 50% 0.91 0.2 0.9 0.81 75% 0.91 0.23 0.94 0.85 max 0.94 0.32 0.96 0.89 computervision - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 1.66 0.86 0.86 std 0.03 0.76 0.06 0.04 min 0.83 0.81 0.77 0.75 25% 0.89 1.14 0.82 0.85 50% 0.91 1.4 0.86 0.87 75% 0.92 2.08 0.91 0.89 max 0.94 3.01 0.95 0.9 computervision - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.27 0.9 0.82 std 0.02 0.06 0.04 0.04 min 0.87 0.17 0.85 0.75 25% 0.89 0.23 0.87 0.8 50% 0.91 0.26 0.89 0.81 75% 0.92 0.3 0.92 0.85 max 0.93 0.39 0.96 0.89 genomeediting - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.91 1.21 0.85 0.91 std 0.02 0.55 0.04 0.03 min 0.87 0.81 0.8 0.86 25% 0.9 0.94 0.81 0.9 50% 0.91 1.03 0.87 0.91 75% 0.93 1.11 0.88 0.94 max 0.94 2.66 0.88 0.96 genomeediting - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.92 0.23 0.89 0.87 std 0.02 0.04 0.03 0.04 min 0.89 0.15 0.84 0.82 25% 0.9 0.21 0.87 0.85 50% 0.92 0.23 0.89 0.87 75% 0.93 0.25 0.91 0.89 max 0.95 0.3 0.94 0.95 hydrogenstorage - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 2.43 0.78 0.84 std 0.04 0.8 0.05 0.07 min 0.81 0.99 0.71 0.72 25% 0.84 2.1 0.75 0.8 50% 0.87 2.41 0.76 0.83 75% 0.88 2.72 0.82 0.89 max 0.94 4.08 0.86 0.94 hydrogenstorage - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 0.35 0.85 0.72 std 0.02 0.04 0.04 0.05 min 0.84 0.25 0.76 0.62 25% 0.86 0.34 0.84 0.71 50% 0.86 0.35 0.86 0.73 75% 0.88 0.37 0.88 0.75 max 0.9 0.42 0.9 0.79 naturallanguageprocessing - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.89 0.97 0.81 0.88 std 0.03 0.2 0.06 0.05 min 0.82 0.62 0.72 0.8 25% 0.88 0.87 0.76 0.86 50% 0.88 0.96 0.83 0.88 75% 0.91 1.14 0.85 0.9 max 0.94 1.23 0.87 0.98 naturallanguageprocessing - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.27 0.89 0.79 std 0.02 0.06 0.03 0.05 min 0.87 0.14 0.84 0.74 25% 0.88 0.24 0.87 0.77 50% 0.89 0.27 0.88 0.78 75% 0.91 0.3 0.91 0.81 max 0.95 0.36 0.95 0.9 selfdrivingvehicle - cnn \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.8 3.6 0.69 0.71 std 0.04 0.74 0.06 0.09 min 0.72 2.57 0.55 0.54 25% 0.78 3.07 0.66 0.65 50% 0.8 3.57 0.69 0.73 75% 0.82 4.02 0.72 0.77 max 0.85 4.85 0.78 0.83 selfdrivingvehicle - mlp \u00b6 binary_accuracy loss precision recall count 10 10 10 10 mean 0.82 0.51 0.8 0.63 std 0.02 0.06 0.06 0.04 min 0.77 0.43 0.71 0.57 25% 0.82 0.47 0.77 0.6 50% 0.82 0.5 0.79 0.65 75% 0.84 0.54 0.82 0.66 max 0.85 0.63 0.94 0.68","title":"Performance"},{"location":"MODELS_PERFORMANCE/#models-performance","text":"","title":"MODELS PERFORMANCE"},{"location":"MODELS_PERFORMANCE/#additivemanufacturing-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 2.46 0.78 0.82 std 0.03 0.42 0.07 0.07 min 0.82 1.68 0.69 0.69 25% 0.86 2.22 0.72 0.79 50% 0.88 2.49 0.79 0.85 75% 0.89 2.76 0.84 0.87 max 0.89 3 0.88 0.89","title":"additivemanufacturing - cnn"},{"location":"MODELS_PERFORMANCE/#additivemanufacturing-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.3 0.89 0.79 std 0.02 0.05 0.04 0.05 min 0.87 0.21 0.82 0.72 25% 0.89 0.26 0.87 0.75 50% 0.9 0.31 0.89 0.79 75% 0.91 0.34 0.92 0.81 max 0.93 0.37 0.95 0.88","title":"additivemanufacturing - mlp"},{"location":"MODELS_PERFORMANCE/#blockchain-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 1.11 0.84 0.89 std 0.03 0.51 0.05 0.06 min 0.85 0.34 0.76 0.8 25% 0.89 0.79 0.82 0.85 50% 0.91 1.12 0.83 0.88 75% 0.92 1.39 0.87 0.94 max 0.94 2.11 0.9 0.95","title":"blockchain - cnn"},{"location":"MODELS_PERFORMANCE/#blockchain-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.91 0.22 0.91 0.82 std 0.01 0.04 0.03 0.04 min 0.89 0.18 0.87 0.77 25% 0.9 0.19 0.89 0.78 50% 0.91 0.2 0.9 0.81 75% 0.91 0.23 0.94 0.85 max 0.94 0.32 0.96 0.89","title":"blockchain - mlp"},{"location":"MODELS_PERFORMANCE/#computervision-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 1.66 0.86 0.86 std 0.03 0.76 0.06 0.04 min 0.83 0.81 0.77 0.75 25% 0.89 1.14 0.82 0.85 50% 0.91 1.4 0.86 0.87 75% 0.92 2.08 0.91 0.89 max 0.94 3.01 0.95 0.9","title":"computervision - cnn"},{"location":"MODELS_PERFORMANCE/#computervision-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.27 0.9 0.82 std 0.02 0.06 0.04 0.04 min 0.87 0.17 0.85 0.75 25% 0.89 0.23 0.87 0.8 50% 0.91 0.26 0.89 0.81 75% 0.92 0.3 0.92 0.85 max 0.93 0.39 0.96 0.89","title":"computervision - mlp"},{"location":"MODELS_PERFORMANCE/#genomeediting-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.91 1.21 0.85 0.91 std 0.02 0.55 0.04 0.03 min 0.87 0.81 0.8 0.86 25% 0.9 0.94 0.81 0.9 50% 0.91 1.03 0.87 0.91 75% 0.93 1.11 0.88 0.94 max 0.94 2.66 0.88 0.96","title":"genomeediting - cnn"},{"location":"MODELS_PERFORMANCE/#genomeediting-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.92 0.23 0.89 0.87 std 0.02 0.04 0.03 0.04 min 0.89 0.15 0.84 0.82 25% 0.9 0.21 0.87 0.85 50% 0.92 0.23 0.89 0.87 75% 0.93 0.25 0.91 0.89 max 0.95 0.3 0.94 0.95","title":"genomeediting - mlp"},{"location":"MODELS_PERFORMANCE/#hydrogenstorage-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 2.43 0.78 0.84 std 0.04 0.8 0.05 0.07 min 0.81 0.99 0.71 0.72 25% 0.84 2.1 0.75 0.8 50% 0.87 2.41 0.76 0.83 75% 0.88 2.72 0.82 0.89 max 0.94 4.08 0.86 0.94","title":"hydrogenstorage - cnn"},{"location":"MODELS_PERFORMANCE/#hydrogenstorage-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.87 0.35 0.85 0.72 std 0.02 0.04 0.04 0.05 min 0.84 0.25 0.76 0.62 25% 0.86 0.34 0.84 0.71 50% 0.86 0.35 0.86 0.73 75% 0.88 0.37 0.88 0.75 max 0.9 0.42 0.9 0.79","title":"hydrogenstorage - mlp"},{"location":"MODELS_PERFORMANCE/#naturallanguageprocessing-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.89 0.97 0.81 0.88 std 0.03 0.2 0.06 0.05 min 0.82 0.62 0.72 0.8 25% 0.88 0.87 0.76 0.86 50% 0.88 0.96 0.83 0.88 75% 0.91 1.14 0.85 0.9 max 0.94 1.23 0.87 0.98","title":"naturallanguageprocessing - cnn"},{"location":"MODELS_PERFORMANCE/#naturallanguageprocessing-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.9 0.27 0.89 0.79 std 0.02 0.06 0.03 0.05 min 0.87 0.14 0.84 0.74 25% 0.88 0.24 0.87 0.77 50% 0.89 0.27 0.88 0.78 75% 0.91 0.3 0.91 0.81 max 0.95 0.36 0.95 0.9","title":"naturallanguageprocessing - mlp"},{"location":"MODELS_PERFORMANCE/#selfdrivingvehicle-cnn","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.8 3.6 0.69 0.71 std 0.04 0.74 0.06 0.09 min 0.72 2.57 0.55 0.54 25% 0.78 3.07 0.66 0.65 50% 0.8 3.57 0.69 0.73 75% 0.82 4.02 0.72 0.77 max 0.85 4.85 0.78 0.83","title":"selfdrivingvehicle - cnn"},{"location":"MODELS_PERFORMANCE/#selfdrivingvehicle-mlp","text":"binary_accuracy loss precision recall count 10 10 10 10 mean 0.82 0.51 0.8 0.63 std 0.02 0.06 0.06 0.04 min 0.77 0.43 0.71 0.57 25% 0.82 0.47 0.77 0.6 50% 0.82 0.5 0.79 0.65 75% 0.84 0.54 0.82 0.66 max 0.85 0.63 0.94 0.68","title":"selfdrivingvehicle - mlp"},{"location":"RECIPE/","text":"RECIPE \u00b6 Candidates \u00b6 Generate sample \u00b6 dvc repro candidates Info The annotation sample is randomly drawn from all candidates defined by the config file. Do not expect the exact same outs if you run it twice. Label sample \u00b6 TECHNOLOGY = \"\" # e.g. \"additivemanufacturing\" prodigy textcat.option SEED_ ${ TECHNOLOGY } data/candidates_ ${ TECHNOLOGY } _sample.jsonl -F prodigy/textcat_option.py Load annotated SEED \u00b6 dvc unprotect data/seed*.jsonl dvc repro load-annotated-seed dvc add data/seed*.jsonl Build publications table at family_id level \u00b6 techlandscape assets get-publications-family patentcity.patents.publications credentials_bq.json # fix REPEATED instead of NULLABLE nested field gsutil -m rm \"gs://tmp/publications_*.jsonl.gz\" bq extract --destination_format NEWLINE_DELIMITED_JSON --compression GZIP patentcity:patents.publications \"gs://tmp/publications_*.jsonl.gz\" STAGE_FOLDER = \"\" gsutil -m cp \"gs://tmp/publications_*.jsonl.gz\" $STAGE_FOLDER ls ${ STAGE_FOLDER } /publications_*.jsonl.gz | parallel --eta 'mv {} {.}.tmp.gz && techlandscape utils flatten-nested-vars {.}.tmp.gz cpc,ipc,citation,cited_by >> {.} && gzip {.}' gsutil -m cp \" ${ STAGE_FOLDER } /publications_*.jsonl.gz\" gs://tmp/ bq rm patentcity:patents.publications bq load --source_format NEWLINE_DELIMITED_JSON --replace --max_bad_records 1000 patentcity:patents.publications \"gs://tmp/publications_*.jsonl.gz\" schemas/publications_familyid.json Expansion iteration and robustness analysis \u00b6 dvc unprotect outs/expansion_*robustness*.csv dvc repro -f expansion-robustness dvc add outs/expansion_*robustness*.csv Training data \u00b6 cat lib/technology.txt | parallel --eta 'techlandscape io get-training-data family_id patentcity.techdiffusion.seed_{} patentcity.techdiffusion.expansion_{} 400 patentcity.techdiffusion.training_{} credentials_bq.json' gsutil -m rm \"gs://tmp/training_*.jsonl\" cat lib/technology.txt | parallel --eta 'bq extract --destination_format NEWLINE_DELIMITED_JSON patentcity:techdiffusion.training_{} \"gs://tmp/training_{}.jsonl\" ' gsutil -m cp \"gs://tmp/training_*.jsonl\" data/ Prediction robustness \u00b6 parallel --eta 'techlandscape utils train-test-split data/training_{1}.jsonl data/train_{1}_{2}.jsonl data/test_{1}_{2}.jsonl' ::: $( cat lib/technology.txt ) ::: $( pwgen 5 10 ) parallel --eta 'python techlandscape/model.py +model_{3}=default data.train=data/train_{1}_{2}.jsonl data.test=data/test_{1}_{2}.jsonl out=models/{1}_{2}_{3} logger.tensorboard.logdir=models/{1}_{2}_{3}/log' ::: $( cat lib/technology.txt ) ::: $( ls data/test_*.jsonl | cut -d_ -f3 | cut -d. -f1 | sort -u ) ::: cnn mlp cat lib/technology.txt | parallel --eta 'techlandscape io get-expansion family_id patentcity.techdiffusion.expansion_{} patentcity.stage.expansion_{}_sample credentials_bq.json --sample-size 10000' gsutil -m rm \"gs://tmp/expansion_*_sample.jsonl.gz\" cat lib/technology.txt | parallel --eta 'bq extract --destination_format NEWLINE_DELIMITED_JSON --compression GZIP patentcity:stage.expansion_{}_sample \"gs://tmp/expansion_{}_sample.jsonl.gz\"' gsutil -m cp \"gs://tmp/expansion_*_sample.jsonl.gz\" data/ parallel --eta 'techlandscape robustness get-prediction-analysis \"models/{1}_*_{2}/model-best\" data/expansion_{1}_sample.jsonl --destination outs/' ::: $( cat lib/technology.txt ) ::: $( cat lib/model.txt ) techlandscape robustness wrap-prediction-analysis \"outs/classification_*.csv\" >> docs/ROBUSTNESS_MODEL.md parallel --eta -j1 'techlandscape robustness models-performance \"models/{1}_*_{2}/model-best/meta.json\" --markdown --title \"{1} - {2}\"' ::: $( cat lib/technology.txt ) ::: $( cat lib/model.txt ) >> docs/MODELS_PERFORMANCE.md","title":"Recipes"},{"location":"RECIPE/#recipe","text":"","title":"RECIPE"},{"location":"RECIPE/#candidates","text":"","title":"Candidates"},{"location":"RECIPE/#generate-sample","text":"dvc repro candidates Info The annotation sample is randomly drawn from all candidates defined by the config file. Do not expect the exact same outs if you run it twice.","title":"Generate sample"},{"location":"RECIPE/#label-sample","text":"TECHNOLOGY = \"\" # e.g. \"additivemanufacturing\" prodigy textcat.option SEED_ ${ TECHNOLOGY } data/candidates_ ${ TECHNOLOGY } _sample.jsonl -F prodigy/textcat_option.py","title":"Label sample"},{"location":"RECIPE/#load-annotated-seed","text":"dvc unprotect data/seed*.jsonl dvc repro load-annotated-seed dvc add data/seed*.jsonl","title":"Load annotated SEED"},{"location":"RECIPE/#build-publications-table-at-family_id-level","text":"techlandscape assets get-publications-family patentcity.patents.publications credentials_bq.json # fix REPEATED instead of NULLABLE nested field gsutil -m rm \"gs://tmp/publications_*.jsonl.gz\" bq extract --destination_format NEWLINE_DELIMITED_JSON --compression GZIP patentcity:patents.publications \"gs://tmp/publications_*.jsonl.gz\" STAGE_FOLDER = \"\" gsutil -m cp \"gs://tmp/publications_*.jsonl.gz\" $STAGE_FOLDER ls ${ STAGE_FOLDER } /publications_*.jsonl.gz | parallel --eta 'mv {} {.}.tmp.gz && techlandscape utils flatten-nested-vars {.}.tmp.gz cpc,ipc,citation,cited_by >> {.} && gzip {.}' gsutil -m cp \" ${ STAGE_FOLDER } /publications_*.jsonl.gz\" gs://tmp/ bq rm patentcity:patents.publications bq load --source_format NEWLINE_DELIMITED_JSON --replace --max_bad_records 1000 patentcity:patents.publications \"gs://tmp/publications_*.jsonl.gz\" schemas/publications_familyid.json","title":"Build publications table at family_id level"},{"location":"RECIPE/#expansion-iteration-and-robustness-analysis","text":"dvc unprotect outs/expansion_*robustness*.csv dvc repro -f expansion-robustness dvc add outs/expansion_*robustness*.csv","title":"Expansion iteration and robustness analysis"},{"location":"RECIPE/#training-data","text":"cat lib/technology.txt | parallel --eta 'techlandscape io get-training-data family_id patentcity.techdiffusion.seed_{} patentcity.techdiffusion.expansion_{} 400 patentcity.techdiffusion.training_{} credentials_bq.json' gsutil -m rm \"gs://tmp/training_*.jsonl\" cat lib/technology.txt | parallel --eta 'bq extract --destination_format NEWLINE_DELIMITED_JSON patentcity:techdiffusion.training_{} \"gs://tmp/training_{}.jsonl\" ' gsutil -m cp \"gs://tmp/training_*.jsonl\" data/","title":"Training data"},{"location":"RECIPE/#prediction-robustness","text":"parallel --eta 'techlandscape utils train-test-split data/training_{1}.jsonl data/train_{1}_{2}.jsonl data/test_{1}_{2}.jsonl' ::: $( cat lib/technology.txt ) ::: $( pwgen 5 10 ) parallel --eta 'python techlandscape/model.py +model_{3}=default data.train=data/train_{1}_{2}.jsonl data.test=data/test_{1}_{2}.jsonl out=models/{1}_{2}_{3} logger.tensorboard.logdir=models/{1}_{2}_{3}/log' ::: $( cat lib/technology.txt ) ::: $( ls data/test_*.jsonl | cut -d_ -f3 | cut -d. -f1 | sort -u ) ::: cnn mlp cat lib/technology.txt | parallel --eta 'techlandscape io get-expansion family_id patentcity.techdiffusion.expansion_{} patentcity.stage.expansion_{}_sample credentials_bq.json --sample-size 10000' gsutil -m rm \"gs://tmp/expansion_*_sample.jsonl.gz\" cat lib/technology.txt | parallel --eta 'bq extract --destination_format NEWLINE_DELIMITED_JSON --compression GZIP patentcity:stage.expansion_{}_sample \"gs://tmp/expansion_{}_sample.jsonl.gz\"' gsutil -m cp \"gs://tmp/expansion_*_sample.jsonl.gz\" data/ parallel --eta 'techlandscape robustness get-prediction-analysis \"models/{1}_*_{2}/model-best\" data/expansion_{1}_sample.jsonl --destination outs/' ::: $( cat lib/technology.txt ) ::: $( cat lib/model.txt ) techlandscape robustness wrap-prediction-analysis \"outs/classification_*.csv\" >> docs/ROBUSTNESS_MODEL.md parallel --eta -j1 'techlandscape robustness models-performance \"models/{1}_*_{2}/model-best/meta.json\" --markdown --title \"{1} - {2}\"' ::: $( cat lib/technology.txt ) ::: $( cat lib/model.txt ) >> docs/MODELS_PERFORMANCE.md","title":"Prediction robustness"},{"location":"ROBUSTNESS_EXPANSION/","text":"EXPANSION ROBUSTNESS \u00b6 Question \u00b6 How do random variations in the seed affect the overall expansion? If small variations in the seed (sample uncertainty) largely affect the overall expansion, this raises consistency issues. Related question: How to measure robustness of the expansion step? Approach \u00b6 We emulate data uncertainty by generating a sequence of random draws in the annotated seed. Then, we can compare the generated expansions and look at how data uncertainty affect the overall expansion. Using the generated expansions, we look at two things: pairwise overlap: for all pairs of expansions (k in n), we compute the share of families which are in both expansions and report moments of the distribution batch overlap: for all expansions , we compute the share of families which are in all expansions and report moments of the distribution Results \u00b6 Overall, we find that the expansion is robust to data uncertainty, even under particularly conservative conditions. Drawing 10 different random samples of the seed (50% of the seed), the resulting expansions exhibit a median family overlap ranging between 76% and 94%. The median (and other moments) of the overlap distribution grow as the share grows - as expected. Code dvc unprotect outs/expansion_*robustness*.csv dvc repro -f expansion-robustness dvc add outs/expansion_*robustness*.csv techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" technologies --markdown additivemanufacturing \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.76 0.08 0.62 0.71 0.76 0.82 0.86 batchrobustness0.7 10 0.78 0.08 0.69 0.72 0.75 0.83 0.93 batchrobustness0.9 10 0.79 0.06 0.71 0.78 0.79 0.79 0.94 pairwiserobustness0.5 90 0.88 0.07 0.69 0.83 0.89 0.94 0.98 pairwiserobustness0.7 90 0.91 0.07 0.72 0.88 0.93 0.97 0.99 pairwiserobustness0.9 90 0.95 0.06 0.75 0.91 0.99 0.99 1 blockchain \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.91 0.01 0.9 0.9 0.91 0.92 0.93 batchrobustness0.7 10 0.94 0.01 0.92 0.93 0.94 0.94 0.95 batchrobustness0.9 10 0.96 0 0.96 0.96 0.96 0.97 0.97 pairwiserobustness0.5 90 0.96 0.01 0.93 0.95 0.96 0.97 0.98 pairwiserobustness0.7 90 0.98 0.01 0.96 0.97 0.98 0.98 0.99 pairwiserobustness0.9 90 0.99 0.01 0.97 0.98 0.99 0.99 1 computervision \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.83 0.02 0.79 0.82 0.84 0.85 0.85 batchrobustness0.7 10 0.85 0.01 0.84 0.85 0.85 0.86 0.87 batchrobustness0.9 10 0.95 0 0.94 0.95 0.95 0.95 0.95 pairwiserobustness0.5 90 0.92 0.02 0.87 0.91 0.92 0.94 0.96 pairwiserobustness0.7 90 0.95 0.02 0.91 0.94 0.96 0.96 0.98 pairwiserobustness0.9 90 0.99 0 0.98 0.99 0.99 0.99 1 genomeediting \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.94 0.03 0.9 0.92 0.94 0.97 0.98 batchrobustness0.7 10 0.97 0.01 0.94 0.97 0.97 0.98 0.98 batchrobustness0.9 10 0.96 0.02 0.94 0.94 0.95 0.97 0.98 pairwiserobustness0.5 90 0.97 0.02 0.92 0.95 0.98 0.99 0.99 pairwiserobustness0.7 90 0.99 0.01 0.96 0.99 0.99 0.99 1 pairwiserobustness0.9 90 0.99 0.02 0.95 0.99 0.99 1 1 hydrogenstorage \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.01 0.88 0.89 0.9 0.91 0.91 batchrobustness0.7 10 0.92 0.01 0.91 0.91 0.92 0.92 0.92 batchrobustness0.9 10 0.96 0 0.95 0.96 0.96 0.96 0.97 pairwiserobustness0.5 90 0.95 0.01 0.91 0.95 0.95 0.96 0.98 pairwiserobustness0.7 90 0.97 0.01 0.95 0.97 0.97 0.98 0.99 pairwiserobustness0.9 90 0.99 0.01 0.98 0.98 0.99 0.99 1 naturallanguageprocessing \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.03 0.86 0.87 0.9 0.92 0.96 batchrobustness0.7 10 0.92 0.01 0.91 0.92 0.92 0.92 0.96 batchrobustness0.9 10 0.99 0 0.99 0.99 0.99 0.99 0.99 pairwiserobustness0.5 90 0.96 0.03 0.89 0.94 0.96 0.98 0.99 pairwiserobustness0.7 90 0.98 0.01 0.94 0.98 0.98 0.99 1 pairwiserobustness0.9 90 1 0 0.99 0.99 1 1 1 selfdrivingvehicle \u00b6 configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.02 0.87 0.88 0.89 0.91 0.92 batchrobustness0.7 10 0.91 0.01 0.9 0.91 0.91 0.92 0.93 batchrobustness0.9 10 0.93 0.01 0.92 0.93 0.93 0.94 0.94 pairwiserobustness0.5 90 0.95 0.02 0.91 0.93 0.95 0.96 0.98 pairwiserobustness0.7 90 0.97 0.01 0.93 0.96 0.97 0.97 0.98 pairwiserobustness0.9 90 0.98 0.01 0.97 0.98 0.99 0.99 1","title":"Expansion"},{"location":"ROBUSTNESS_EXPANSION/#expansion-robustness","text":"","title":"EXPANSION ROBUSTNESS"},{"location":"ROBUSTNESS_EXPANSION/#question","text":"How do random variations in the seed affect the overall expansion? If small variations in the seed (sample uncertainty) largely affect the overall expansion, this raises consistency issues. Related question: How to measure robustness of the expansion step?","title":"Question"},{"location":"ROBUSTNESS_EXPANSION/#approach","text":"We emulate data uncertainty by generating a sequence of random draws in the annotated seed. Then, we can compare the generated expansions and look at how data uncertainty affect the overall expansion. Using the generated expansions, we look at two things: pairwise overlap: for all pairs of expansions (k in n), we compute the share of families which are in both expansions and report moments of the distribution batch overlap: for all expansions , we compute the share of families which are in all expansions and report moments of the distribution","title":"Approach"},{"location":"ROBUSTNESS_EXPANSION/#results","text":"Overall, we find that the expansion is robust to data uncertainty, even under particularly conservative conditions. Drawing 10 different random samples of the seed (50% of the seed), the resulting expansions exhibit a median family overlap ranging between 76% and 94%. The median (and other moments) of the overlap distribution grow as the share grows - as expected. Code dvc unprotect outs/expansion_*robustness*.csv dvc repro -f expansion-robustness dvc add outs/expansion_*robustness*.csv techlandscape robustness wrap-overlap-analysis \"outs/expansion_*robustness*.csv\" technologies --markdown","title":"Results"},{"location":"ROBUSTNESS_EXPANSION/#additivemanufacturing","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.76 0.08 0.62 0.71 0.76 0.82 0.86 batchrobustness0.7 10 0.78 0.08 0.69 0.72 0.75 0.83 0.93 batchrobustness0.9 10 0.79 0.06 0.71 0.78 0.79 0.79 0.94 pairwiserobustness0.5 90 0.88 0.07 0.69 0.83 0.89 0.94 0.98 pairwiserobustness0.7 90 0.91 0.07 0.72 0.88 0.93 0.97 0.99 pairwiserobustness0.9 90 0.95 0.06 0.75 0.91 0.99 0.99 1","title":"additivemanufacturing"},{"location":"ROBUSTNESS_EXPANSION/#blockchain","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.91 0.01 0.9 0.9 0.91 0.92 0.93 batchrobustness0.7 10 0.94 0.01 0.92 0.93 0.94 0.94 0.95 batchrobustness0.9 10 0.96 0 0.96 0.96 0.96 0.97 0.97 pairwiserobustness0.5 90 0.96 0.01 0.93 0.95 0.96 0.97 0.98 pairwiserobustness0.7 90 0.98 0.01 0.96 0.97 0.98 0.98 0.99 pairwiserobustness0.9 90 0.99 0.01 0.97 0.98 0.99 0.99 1","title":"blockchain"},{"location":"ROBUSTNESS_EXPANSION/#computervision","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.83 0.02 0.79 0.82 0.84 0.85 0.85 batchrobustness0.7 10 0.85 0.01 0.84 0.85 0.85 0.86 0.87 batchrobustness0.9 10 0.95 0 0.94 0.95 0.95 0.95 0.95 pairwiserobustness0.5 90 0.92 0.02 0.87 0.91 0.92 0.94 0.96 pairwiserobustness0.7 90 0.95 0.02 0.91 0.94 0.96 0.96 0.98 pairwiserobustness0.9 90 0.99 0 0.98 0.99 0.99 0.99 1","title":"computervision"},{"location":"ROBUSTNESS_EXPANSION/#genomeediting","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.94 0.03 0.9 0.92 0.94 0.97 0.98 batchrobustness0.7 10 0.97 0.01 0.94 0.97 0.97 0.98 0.98 batchrobustness0.9 10 0.96 0.02 0.94 0.94 0.95 0.97 0.98 pairwiserobustness0.5 90 0.97 0.02 0.92 0.95 0.98 0.99 0.99 pairwiserobustness0.7 90 0.99 0.01 0.96 0.99 0.99 0.99 1 pairwiserobustness0.9 90 0.99 0.02 0.95 0.99 0.99 1 1","title":"genomeediting"},{"location":"ROBUSTNESS_EXPANSION/#hydrogenstorage","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.01 0.88 0.89 0.9 0.91 0.91 batchrobustness0.7 10 0.92 0.01 0.91 0.91 0.92 0.92 0.92 batchrobustness0.9 10 0.96 0 0.95 0.96 0.96 0.96 0.97 pairwiserobustness0.5 90 0.95 0.01 0.91 0.95 0.95 0.96 0.98 pairwiserobustness0.7 90 0.97 0.01 0.95 0.97 0.97 0.98 0.99 pairwiserobustness0.9 90 0.99 0.01 0.98 0.98 0.99 0.99 1","title":"hydrogenstorage"},{"location":"ROBUSTNESS_EXPANSION/#naturallanguageprocessing","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.03 0.86 0.87 0.9 0.92 0.96 batchrobustness0.7 10 0.92 0.01 0.91 0.92 0.92 0.92 0.96 batchrobustness0.9 10 0.99 0 0.99 0.99 0.99 0.99 0.99 pairwiserobustness0.5 90 0.96 0.03 0.89 0.94 0.96 0.98 0.99 pairwiserobustness0.7 90 0.98 0.01 0.94 0.98 0.98 0.99 1 pairwiserobustness0.9 90 1 0 0.99 0.99 1 1 1","title":"naturallanguageprocessing"},{"location":"ROBUSTNESS_EXPANSION/#selfdrivingvehicle","text":"configs count mean std min 25% 50% 75% max batchrobustness0.5 10 0.9 0.02 0.87 0.88 0.89 0.91 0.92 batchrobustness0.7 10 0.91 0.01 0.9 0.91 0.91 0.92 0.93 batchrobustness0.9 10 0.93 0.01 0.92 0.93 0.93 0.94 0.94 pairwiserobustness0.5 90 0.95 0.02 0.91 0.93 0.95 0.96 0.98 pairwiserobustness0.7 90 0.97 0.01 0.93 0.96 0.97 0.97 0.98 pairwiserobustness0.9 90 0.98 0.01 0.97 0.98 0.99 0.99 1","title":"selfdrivingvehicle"},{"location":"ROBUSTNESS_MODEL/","text":"MODEL ROBUSTNESS \u00b6 Question \u00b6 How random variations in the seed and the antiseed affect the pruning step. Approach \u00b6 We emulate random variations by iterating over various training/test set randomly drawn from the same set of annotated data. We define a default model architecture, train as many models as training/test sets and compare the predictions of the different models out of sample (10,000) on a common benchmark of families/publications drawn from the expansion set. Specifically, we look at: moments of the distribution of standard errors of models predictions (std computed at the family/publication level) models' consensus regarding positive and negative examples Results \u00b6 additivemanufacturing - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.09 std 0.054 min 0 25% 0.055 50% 0.082 75% 0.113 max 0.384 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 9 747 0.145 0.145 8 520 0.101 0.246 7 595 0.115 0.361 6 486 0.094 0.455 5 510 0.099 0.554 4 560 0.109 0.663 3 603 0.117 0.78 2 536 0.104 0.884 1 600 0.116 1 0 0 0 1 additivemanufacturing - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.041 std 0.039 min 0 25% 0.014 50% 0.029 75% 0.056 max 0.276 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 320 0.225 0.225 9 126 0.089 0.314 8 112 0.079 0.393 7 42 0.03 0.423 6 114 0.08 0.503 5 95 0.067 0.57 4 124 0.087 0.657 3 129 0.091 0.748 2 152 0.107 0.855 1 206 0.145 1 0 0 0 1 blockchain - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.074 std 0.073 min 0 25% 0.012 50% 0.047 75% 0.126 max 0.389 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 410 0.084 0.084 9 360 0.074 0.158 8 352 0.072 0.23 7 336 0.069 0.299 6 360 0.074 0.373 5 390 0.08 0.453 4 380 0.078 0.531 3 357 0.073 0.604 2 1410 0.289 0.893 1 522 0.107 1 0 0 0 1 blockchain - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.025 std 0.04 min 0 25% 0.003 50% 0.008 75% 0.03 max 0.272 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 160 0.179 0.179 9 18 0.02 0.199 8 24 0.027 0.226 7 42 0.047 0.273 6 66 0.074 0.346 5 125 0.14 0.486 4 56 0.063 0.549 3 105 0.117 0.666 2 122 0.136 0.802 1 177 0.198 1 0 0 0 1 computervision - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.065 std 0.074 min 0 25% 0.003 50% 0.029 75% 0.121 max 0.383 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 3590 0.381 0.381 9 810 0.086 0.467 8 504 0.054 0.521 7 483 0.051 0.572 6 468 0.05 0.622 5 400 0.042 0.664 4 340 0.036 0.7 3 330 0.035 0.735 2 2072 0.22 0.955 1 420 0.045 1 0 0 0 1 computervision - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.035 std 0.046 min 0 25% 0.006 50% 0.015 75% 0.042 max 0.291 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 3310 0.55 0.55 9 684 0.114 0.664 8 360 0.06 0.724 7 357 0.059 0.783 6 300 0.05 0.833 5 225 0.037 0.87 4 212 0.035 0.906 3 204 0.034 0.939 2 180 0.03 0.969 1 184 0.031 1 0 0 0 1 genomeediting - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.03 std 0.053 min 0 25% 0 50% 0.001 75% 0.023 max 0.329 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 450 0.126 0.126 9 90 0.025 0.152 8 88 0.025 0.176 7 70 0.02 0.196 6 96 0.027 0.223 5 90 0.025 0.248 4 280 0.079 0.327 3 1866 0.524 0.851 2 356 0.1 0.951 1 175 0.049 1 0 0 0 1 genomeediting - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.011 std 0.021 min 0 25% 0.001 50% 0.003 75% 0.01 max 0.205 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 360 0.462 0.462 9 63 0.081 0.543 8 56 0.072 0.615 7 28 0.036 0.651 6 60 0.077 0.728 5 40 0.051 0.779 4 60 0.077 0.856 3 33 0.042 0.899 2 48 0.062 0.96 1 31 0.04 1 0 0 0 1 hydrogenstorage - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.046 std 0.04 min 0 25% 0.011 50% 0.037 75% 0.076 max 0.311 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 230 0.213 0.213 9 117 0.108 0.321 8 120 0.111 0.432 7 91 0.084 0.516 6 90 0.083 0.599 5 75 0.069 0.669 4 100 0.093 0.761 3 72 0.067 0.828 2 84 0.078 0.906 1 102 0.094 1 0 0 0 1 hydrogenstorage - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.026 std 0.031 min 0.001 25% 0.007 50% 0.015 75% 0.03 max 0.257 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 160 0.21 0.21 9 54 0.071 0.281 8 40 0.052 0.333 7 63 0.083 0.416 6 78 0.102 0.518 5 65 0.085 0.604 4 80 0.105 0.709 3 60 0.079 0.787 2 72 0.094 0.882 1 90 0.118 1 0 0 0 1 naturallanguageprocessing - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.058 std 0.072 min 0 25% 0.001 50% 0.018 75% 0.114 max 0.39 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 1630 0.199 0.199 9 666 0.081 0.281 8 520 0.064 0.344 7 301 0.037 0.381 6 354 0.043 0.424 5 475 0.058 0.482 4 2972 0.363 0.846 3 525 0.064 0.91 2 384 0.047 0.957 1 353 0.043 1 0 0 0 1 naturallanguageprocessing - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.028 std 0.036 min 0 25% 0.004 50% 0.012 75% 0.036 max 0.339 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 950 0.368 0.368 9 315 0.122 0.49 8 216 0.084 0.574 7 231 0.09 0.663 6 204 0.079 0.742 5 170 0.066 0.808 4 120 0.046 0.855 3 96 0.037 0.892 2 126 0.049 0.941 1 153 0.059 1 0 0 0 1 selfdrivingvehicle - cnn \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.085 std 0.044 min 0 25% 0.052 50% 0.091 75% 0.111 max 0.324 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 610 0.086 0.086 9 765 0.107 0.193 8 504 0.071 0.263 7 483 0.068 0.331 6 468 0.066 0.397 5 390 0.055 0.451 4 488 0.068 0.52 3 423 0.059 0.579 2 2292 0.321 0.9 1 711 0.1 1 0 0 0 1 selfdrivingvehicle - mlp \u00b6 Score dispersion \u00b6 std_score count 10000 mean 0.049 std 0.036 min 0.002 25% 0.021 50% 0.039 75% 0.069 max 0.278 Models consensus \u00b6 nb_models nb_positives share_positives cumshare_positives 10 1210 0.302 0.302 9 414 0.104 0.406 8 336 0.084 0.49 7 294 0.074 0.564 6 276 0.069 0.633 5 335 0.084 0.716 4 308 0.077 0.793 3 243 0.061 0.854 2 266 0.066 0.92 1 318 0.08 1 0 0 0 1","title":"Pruning"},{"location":"ROBUSTNESS_MODEL/#model-robustness","text":"","title":"MODEL ROBUSTNESS"},{"location":"ROBUSTNESS_MODEL/#question","text":"How random variations in the seed and the antiseed affect the pruning step.","title":"Question"},{"location":"ROBUSTNESS_MODEL/#approach","text":"We emulate random variations by iterating over various training/test set randomly drawn from the same set of annotated data. We define a default model architecture, train as many models as training/test sets and compare the predictions of the different models out of sample (10,000) on a common benchmark of families/publications drawn from the expansion set. Specifically, we look at: moments of the distribution of standard errors of models predictions (std computed at the family/publication level) models' consensus regarding positive and negative examples","title":"Approach"},{"location":"ROBUSTNESS_MODEL/#results","text":"","title":"Results"},{"location":"ROBUSTNESS_MODEL/#additivemanufacturing-cnn","text":"","title":"additivemanufacturing - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion","text":"std_score count 10000 mean 0.09 std 0.054 min 0 25% 0.055 50% 0.082 75% 0.113 max 0.384","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus","text":"nb_models nb_positives share_positives cumshare_positives 9 747 0.145 0.145 8 520 0.101 0.246 7 595 0.115 0.361 6 486 0.094 0.455 5 510 0.099 0.554 4 560 0.109 0.663 3 603 0.117 0.78 2 536 0.104 0.884 1 600 0.116 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#additivemanufacturing-mlp","text":"","title":"additivemanufacturing - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_1","text":"std_score count 10000 mean 0.041 std 0.039 min 0 25% 0.014 50% 0.029 75% 0.056 max 0.276","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_1","text":"nb_models nb_positives share_positives cumshare_positives 10 320 0.225 0.225 9 126 0.089 0.314 8 112 0.079 0.393 7 42 0.03 0.423 6 114 0.08 0.503 5 95 0.067 0.57 4 124 0.087 0.657 3 129 0.091 0.748 2 152 0.107 0.855 1 206 0.145 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#blockchain-cnn","text":"","title":"blockchain - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_2","text":"std_score count 10000 mean 0.074 std 0.073 min 0 25% 0.012 50% 0.047 75% 0.126 max 0.389","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_2","text":"nb_models nb_positives share_positives cumshare_positives 10 410 0.084 0.084 9 360 0.074 0.158 8 352 0.072 0.23 7 336 0.069 0.299 6 360 0.074 0.373 5 390 0.08 0.453 4 380 0.078 0.531 3 357 0.073 0.604 2 1410 0.289 0.893 1 522 0.107 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#blockchain-mlp","text":"","title":"blockchain - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_3","text":"std_score count 10000 mean 0.025 std 0.04 min 0 25% 0.003 50% 0.008 75% 0.03 max 0.272","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_3","text":"nb_models nb_positives share_positives cumshare_positives 10 160 0.179 0.179 9 18 0.02 0.199 8 24 0.027 0.226 7 42 0.047 0.273 6 66 0.074 0.346 5 125 0.14 0.486 4 56 0.063 0.549 3 105 0.117 0.666 2 122 0.136 0.802 1 177 0.198 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#computervision-cnn","text":"","title":"computervision - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_4","text":"std_score count 10000 mean 0.065 std 0.074 min 0 25% 0.003 50% 0.029 75% 0.121 max 0.383","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_4","text":"nb_models nb_positives share_positives cumshare_positives 10 3590 0.381 0.381 9 810 0.086 0.467 8 504 0.054 0.521 7 483 0.051 0.572 6 468 0.05 0.622 5 400 0.042 0.664 4 340 0.036 0.7 3 330 0.035 0.735 2 2072 0.22 0.955 1 420 0.045 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#computervision-mlp","text":"","title":"computervision - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_5","text":"std_score count 10000 mean 0.035 std 0.046 min 0 25% 0.006 50% 0.015 75% 0.042 max 0.291","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_5","text":"nb_models nb_positives share_positives cumshare_positives 10 3310 0.55 0.55 9 684 0.114 0.664 8 360 0.06 0.724 7 357 0.059 0.783 6 300 0.05 0.833 5 225 0.037 0.87 4 212 0.035 0.906 3 204 0.034 0.939 2 180 0.03 0.969 1 184 0.031 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#genomeediting-cnn","text":"","title":"genomeediting - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_6","text":"std_score count 10000 mean 0.03 std 0.053 min 0 25% 0 50% 0.001 75% 0.023 max 0.329","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_6","text":"nb_models nb_positives share_positives cumshare_positives 10 450 0.126 0.126 9 90 0.025 0.152 8 88 0.025 0.176 7 70 0.02 0.196 6 96 0.027 0.223 5 90 0.025 0.248 4 280 0.079 0.327 3 1866 0.524 0.851 2 356 0.1 0.951 1 175 0.049 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#genomeediting-mlp","text":"","title":"genomeediting - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_7","text":"std_score count 10000 mean 0.011 std 0.021 min 0 25% 0.001 50% 0.003 75% 0.01 max 0.205","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_7","text":"nb_models nb_positives share_positives cumshare_positives 10 360 0.462 0.462 9 63 0.081 0.543 8 56 0.072 0.615 7 28 0.036 0.651 6 60 0.077 0.728 5 40 0.051 0.779 4 60 0.077 0.856 3 33 0.042 0.899 2 48 0.062 0.96 1 31 0.04 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#hydrogenstorage-cnn","text":"","title":"hydrogenstorage - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_8","text":"std_score count 10000 mean 0.046 std 0.04 min 0 25% 0.011 50% 0.037 75% 0.076 max 0.311","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_8","text":"nb_models nb_positives share_positives cumshare_positives 10 230 0.213 0.213 9 117 0.108 0.321 8 120 0.111 0.432 7 91 0.084 0.516 6 90 0.083 0.599 5 75 0.069 0.669 4 100 0.093 0.761 3 72 0.067 0.828 2 84 0.078 0.906 1 102 0.094 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#hydrogenstorage-mlp","text":"","title":"hydrogenstorage - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_9","text":"std_score count 10000 mean 0.026 std 0.031 min 0.001 25% 0.007 50% 0.015 75% 0.03 max 0.257","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_9","text":"nb_models nb_positives share_positives cumshare_positives 10 160 0.21 0.21 9 54 0.071 0.281 8 40 0.052 0.333 7 63 0.083 0.416 6 78 0.102 0.518 5 65 0.085 0.604 4 80 0.105 0.709 3 60 0.079 0.787 2 72 0.094 0.882 1 90 0.118 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#naturallanguageprocessing-cnn","text":"","title":"naturallanguageprocessing - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_10","text":"std_score count 10000 mean 0.058 std 0.072 min 0 25% 0.001 50% 0.018 75% 0.114 max 0.39","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_10","text":"nb_models nb_positives share_positives cumshare_positives 10 1630 0.199 0.199 9 666 0.081 0.281 8 520 0.064 0.344 7 301 0.037 0.381 6 354 0.043 0.424 5 475 0.058 0.482 4 2972 0.363 0.846 3 525 0.064 0.91 2 384 0.047 0.957 1 353 0.043 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#naturallanguageprocessing-mlp","text":"","title":"naturallanguageprocessing - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_11","text":"std_score count 10000 mean 0.028 std 0.036 min 0 25% 0.004 50% 0.012 75% 0.036 max 0.339","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_11","text":"nb_models nb_positives share_positives cumshare_positives 10 950 0.368 0.368 9 315 0.122 0.49 8 216 0.084 0.574 7 231 0.09 0.663 6 204 0.079 0.742 5 170 0.066 0.808 4 120 0.046 0.855 3 96 0.037 0.892 2 126 0.049 0.941 1 153 0.059 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#selfdrivingvehicle-cnn","text":"","title":"selfdrivingvehicle - cnn"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_12","text":"std_score count 10000 mean 0.085 std 0.044 min 0 25% 0.052 50% 0.091 75% 0.111 max 0.324","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_12","text":"nb_models nb_positives share_positives cumshare_positives 10 610 0.086 0.086 9 765 0.107 0.193 8 504 0.071 0.263 7 483 0.068 0.331 6 468 0.066 0.397 5 390 0.055 0.451 4 488 0.068 0.52 3 423 0.059 0.579 2 2292 0.321 0.9 1 711 0.1 1 0 0 0 1","title":"Models consensus"},{"location":"ROBUSTNESS_MODEL/#selfdrivingvehicle-mlp","text":"","title":"selfdrivingvehicle - mlp"},{"location":"ROBUSTNESS_MODEL/#score-dispersion_13","text":"std_score count 10000 mean 0.049 std 0.036 min 0.002 25% 0.021 50% 0.039 75% 0.069 max 0.278","title":"Score dispersion"},{"location":"ROBUSTNESS_MODEL/#models-consensus_13","text":"nb_models nb_positives share_positives cumshare_positives 10 1210 0.302 0.302 9 414 0.104 0.406 8 336 0.084 0.49 7 294 0.074 0.564 6 276 0.069 0.633 5 335 0.084 0.716 4 308 0.077 0.793 3 243 0.061 0.854 2 266 0.066 0.92 1 318 0.08 1 0 0 0 1","title":"Models consensus"}]}